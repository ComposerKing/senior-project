I0223 11:38:58.197916 27291 caffe.cpp:218] Using GPUs 0
I0223 11:38:58.233333 27291 caffe.cpp:223] GPU 0: Tesla K20c
I0223 11:38:58.754825 27291 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 100
base_lr: 0.01
display: 20
max_iter: 2000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "models/caffenet_proj/caffenet_train"
solver_mode: GPU
device_id: 0
net: "models/caffenet_proj/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I0223 11:38:58.755020 27291 solver.cpp:87] Creating training net from net file: models/caffenet_proj/train_val.prototxt
I0223 11:38:58.780714 27291 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0223 11:38:58.780762 27291 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0223 11:38:58.781074 27291 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "examples/imagenet/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0223 11:38:58.781232 27291 layer_factory.hpp:77] Creating layer data
I0223 11:38:58.781397 27291 db_lmdb.cpp:35] Opened lmdb examples/imagenet/ilsvrc12_train_lmdb
I0223 11:38:58.809061 27291 net.cpp:86] Creating Layer data
I0223 11:38:58.809098 27291 net.cpp:382] data -> data
I0223 11:38:58.809142 27291 net.cpp:382] data -> label
I0223 11:38:58.809170 27291 data_transformer.cpp:25] Loading mean file from: data/ilsvrc12/imagenet_mean.binaryproto
I0223 11:38:58.870247 27291 data_layer.cpp:45] output data size: 256,3,227,227
I0223 11:38:59.292888 27291 net.cpp:124] Setting up data
I0223 11:38:59.292933 27291 net.cpp:131] Top shape: 256 3 227 227 (39574272)
I0223 11:38:59.292943 27291 net.cpp:131] Top shape: 256 (256)
I0223 11:38:59.292948 27291 net.cpp:139] Memory required for data: 158298112
I0223 11:38:59.292963 27291 layer_factory.hpp:77] Creating layer conv1
I0223 11:38:59.292994 27291 net.cpp:86] Creating Layer conv1
I0223 11:38:59.293004 27291 net.cpp:408] conv1 <- data
I0223 11:38:59.293023 27291 net.cpp:382] conv1 -> conv1
I0223 11:39:00.000360 27291 net.cpp:124] Setting up conv1
I0223 11:39:00.000408 27291 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0223 11:39:00.000416 27291 net.cpp:139] Memory required for data: 455667712
I0223 11:39:00.000449 27291 layer_factory.hpp:77] Creating layer relu1
I0223 11:39:00.000468 27291 net.cpp:86] Creating Layer relu1
I0223 11:39:00.000475 27291 net.cpp:408] relu1 <- conv1
I0223 11:39:00.000486 27291 net.cpp:369] relu1 -> conv1 (in-place)
I0223 11:39:00.000902 27291 net.cpp:124] Setting up relu1
I0223 11:39:00.000919 27291 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0223 11:39:00.000926 27291 net.cpp:139] Memory required for data: 753037312
I0223 11:39:00.000932 27291 layer_factory.hpp:77] Creating layer pool1
I0223 11:39:00.000943 27291 net.cpp:86] Creating Layer pool1
I0223 11:39:00.000949 27291 net.cpp:408] pool1 <- conv1
I0223 11:39:00.000958 27291 net.cpp:382] pool1 -> pool1
I0223 11:39:00.001022 27291 net.cpp:124] Setting up pool1
I0223 11:39:00.001039 27291 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0223 11:39:00.001044 27291 net.cpp:139] Memory required for data: 824700928
I0223 11:39:00.001049 27291 layer_factory.hpp:77] Creating layer norm1
I0223 11:39:00.001065 27291 net.cpp:86] Creating Layer norm1
I0223 11:39:00.001080 27291 net.cpp:408] norm1 <- pool1
I0223 11:39:00.001107 27291 net.cpp:382] norm1 -> norm1
I0223 11:39:00.001371 27291 net.cpp:124] Setting up norm1
I0223 11:39:00.001386 27291 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0223 11:39:00.001392 27291 net.cpp:139] Memory required for data: 896364544
I0223 11:39:00.001399 27291 layer_factory.hpp:77] Creating layer conv2
I0223 11:39:00.001418 27291 net.cpp:86] Creating Layer conv2
I0223 11:39:00.001425 27291 net.cpp:408] conv2 <- norm1
I0223 11:39:00.001436 27291 net.cpp:382] conv2 -> conv2
I0223 11:39:00.009143 27291 net.cpp:124] Setting up conv2
I0223 11:39:00.009177 27291 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0223 11:39:00.009183 27291 net.cpp:139] Memory required for data: 1087467520
I0223 11:39:00.009203 27291 layer_factory.hpp:77] Creating layer relu2
I0223 11:39:00.009215 27291 net.cpp:86] Creating Layer relu2
I0223 11:39:00.009222 27291 net.cpp:408] relu2 <- conv2
I0223 11:39:00.009232 27291 net.cpp:369] relu2 -> conv2 (in-place)
I0223 11:39:00.009457 27291 net.cpp:124] Setting up relu2
I0223 11:39:00.009474 27291 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0223 11:39:00.009479 27291 net.cpp:139] Memory required for data: 1278570496
I0223 11:39:00.009485 27291 layer_factory.hpp:77] Creating layer pool2
I0223 11:39:00.009496 27291 net.cpp:86] Creating Layer pool2
I0223 11:39:00.009502 27291 net.cpp:408] pool2 <- conv2
I0223 11:39:00.009511 27291 net.cpp:382] pool2 -> pool2
I0223 11:39:00.009567 27291 net.cpp:124] Setting up pool2
I0223 11:39:00.009579 27291 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0223 11:39:00.009584 27291 net.cpp:139] Memory required for data: 1322872832
I0223 11:39:00.009590 27291 layer_factory.hpp:77] Creating layer norm2
I0223 11:39:00.009604 27291 net.cpp:86] Creating Layer norm2
I0223 11:39:00.009611 27291 net.cpp:408] norm2 <- pool2
I0223 11:39:00.009621 27291 net.cpp:382] norm2 -> norm2
I0223 11:39:00.010047 27291 net.cpp:124] Setting up norm2
I0223 11:39:00.010066 27291 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0223 11:39:00.010071 27291 net.cpp:139] Memory required for data: 1367175168
I0223 11:39:00.010077 27291 layer_factory.hpp:77] Creating layer conv3
I0223 11:39:00.010094 27291 net.cpp:86] Creating Layer conv3
I0223 11:39:00.010102 27291 net.cpp:408] conv3 <- norm2
I0223 11:39:00.010112 27291 net.cpp:382] conv3 -> conv3
I0223 11:39:00.025396 27291 net.cpp:124] Setting up conv3
I0223 11:39:00.025449 27291 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0223 11:39:00.025455 27291 net.cpp:139] Memory required for data: 1433628672
I0223 11:39:00.025475 27291 layer_factory.hpp:77] Creating layer relu3
I0223 11:39:00.025488 27291 net.cpp:86] Creating Layer relu3
I0223 11:39:00.025496 27291 net.cpp:408] relu3 <- conv3
I0223 11:39:00.025506 27291 net.cpp:369] relu3 -> conv3 (in-place)
I0223 11:39:00.025732 27291 net.cpp:124] Setting up relu3
I0223 11:39:00.025746 27291 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0223 11:39:00.025753 27291 net.cpp:139] Memory required for data: 1500082176
I0223 11:39:00.025758 27291 layer_factory.hpp:77] Creating layer conv4
I0223 11:39:00.025776 27291 net.cpp:86] Creating Layer conv4
I0223 11:39:00.025784 27291 net.cpp:408] conv4 <- conv3
I0223 11:39:00.025795 27291 net.cpp:382] conv4 -> conv4
I0223 11:39:00.038836 27291 net.cpp:124] Setting up conv4
I0223 11:39:00.038878 27291 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0223 11:39:00.038885 27291 net.cpp:139] Memory required for data: 1566535680
I0223 11:39:00.038899 27291 layer_factory.hpp:77] Creating layer relu4
I0223 11:39:00.038913 27291 net.cpp:86] Creating Layer relu4
I0223 11:39:00.038920 27291 net.cpp:408] relu4 <- conv4
I0223 11:39:00.038933 27291 net.cpp:369] relu4 -> conv4 (in-place)
I0223 11:39:00.039155 27291 net.cpp:124] Setting up relu4
I0223 11:39:00.039170 27291 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0223 11:39:00.039176 27291 net.cpp:139] Memory required for data: 1632989184
I0223 11:39:00.039182 27291 layer_factory.hpp:77] Creating layer conv5
I0223 11:39:00.039211 27291 net.cpp:86] Creating Layer conv5
I0223 11:39:00.039230 27291 net.cpp:408] conv5 <- conv4
I0223 11:39:00.039242 27291 net.cpp:382] conv5 -> conv5
I0223 11:39:00.048818 27291 net.cpp:124] Setting up conv5
I0223 11:39:00.048851 27291 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0223 11:39:00.048857 27291 net.cpp:139] Memory required for data: 1677291520
I0223 11:39:00.048878 27291 layer_factory.hpp:77] Creating layer relu5
I0223 11:39:00.048890 27291 net.cpp:86] Creating Layer relu5
I0223 11:39:00.048897 27291 net.cpp:408] relu5 <- conv5
I0223 11:39:00.048909 27291 net.cpp:369] relu5 -> conv5 (in-place)
I0223 11:39:00.049132 27291 net.cpp:124] Setting up relu5
I0223 11:39:00.049147 27291 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0223 11:39:00.049154 27291 net.cpp:139] Memory required for data: 1721593856
I0223 11:39:00.049160 27291 layer_factory.hpp:77] Creating layer pool5
I0223 11:39:00.049170 27291 net.cpp:86] Creating Layer pool5
I0223 11:39:00.049176 27291 net.cpp:408] pool5 <- conv5
I0223 11:39:00.049187 27291 net.cpp:382] pool5 -> pool5
I0223 11:39:00.049247 27291 net.cpp:124] Setting up pool5
I0223 11:39:00.049259 27291 net.cpp:131] Top shape: 256 256 6 6 (2359296)
I0223 11:39:00.049265 27291 net.cpp:139] Memory required for data: 1731031040
I0223 11:39:00.049271 27291 layer_factory.hpp:77] Creating layer fc6
I0223 11:39:00.049288 27291 net.cpp:86] Creating Layer fc6
I0223 11:39:00.049295 27291 net.cpp:408] fc6 <- pool5
I0223 11:39:00.049304 27291 net.cpp:382] fc6 -> fc6
I0223 11:39:00.638916 27291 net.cpp:124] Setting up fc6
I0223 11:39:00.638963 27291 net.cpp:131] Top shape: 256 4096 (1048576)
I0223 11:39:00.638969 27291 net.cpp:139] Memory required for data: 1735225344
I0223 11:39:00.638985 27291 layer_factory.hpp:77] Creating layer relu6
I0223 11:39:00.638999 27291 net.cpp:86] Creating Layer relu6
I0223 11:39:00.639008 27291 net.cpp:408] relu6 <- fc6
I0223 11:39:00.639019 27291 net.cpp:369] relu6 -> fc6 (in-place)
I0223 11:39:00.639595 27291 net.cpp:124] Setting up relu6
I0223 11:39:00.639611 27291 net.cpp:131] Top shape: 256 4096 (1048576)
I0223 11:39:00.639617 27291 net.cpp:139] Memory required for data: 1739419648
I0223 11:39:00.639623 27291 layer_factory.hpp:77] Creating layer drop6
I0223 11:39:00.639637 27291 net.cpp:86] Creating Layer drop6
I0223 11:39:00.639643 27291 net.cpp:408] drop6 <- fc6
I0223 11:39:00.639652 27291 net.cpp:369] drop6 -> fc6 (in-place)
I0223 11:39:00.639693 27291 net.cpp:124] Setting up drop6
I0223 11:39:00.639704 27291 net.cpp:131] Top shape: 256 4096 (1048576)
I0223 11:39:00.639709 27291 net.cpp:139] Memory required for data: 1743613952
I0223 11:39:00.639715 27291 layer_factory.hpp:77] Creating layer fc7
I0223 11:39:00.639727 27291 net.cpp:86] Creating Layer fc7
I0223 11:39:00.639734 27291 net.cpp:408] fc7 <- fc6
I0223 11:39:00.639745 27291 net.cpp:382] fc7 -> fc7
I0223 11:39:00.901700 27291 net.cpp:124] Setting up fc7
I0223 11:39:00.901747 27291 net.cpp:131] Top shape: 256 4096 (1048576)
I0223 11:39:00.901753 27291 net.cpp:139] Memory required for data: 1747808256
I0223 11:39:00.901768 27291 layer_factory.hpp:77] Creating layer relu7
I0223 11:39:00.901783 27291 net.cpp:86] Creating Layer relu7
I0223 11:39:00.901793 27291 net.cpp:408] relu7 <- fc7
I0223 11:39:00.901804 27291 net.cpp:369] relu7 -> fc7 (in-place)
I0223 11:39:00.902097 27291 net.cpp:124] Setting up relu7
I0223 11:39:00.902112 27291 net.cpp:131] Top shape: 256 4096 (1048576)
I0223 11:39:00.902117 27291 net.cpp:139] Memory required for data: 1752002560
I0223 11:39:00.902123 27291 layer_factory.hpp:77] Creating layer drop7
I0223 11:39:00.902137 27291 net.cpp:86] Creating Layer drop7
I0223 11:39:00.902143 27291 net.cpp:408] drop7 <- fc7
I0223 11:39:00.902151 27291 net.cpp:369] drop7 -> fc7 (in-place)
I0223 11:39:00.902182 27291 net.cpp:124] Setting up drop7
I0223 11:39:00.902192 27291 net.cpp:131] Top shape: 256 4096 (1048576)
I0223 11:39:00.902199 27291 net.cpp:139] Memory required for data: 1756196864
I0223 11:39:00.902204 27291 layer_factory.hpp:77] Creating layer fc8
I0223 11:39:00.902220 27291 net.cpp:86] Creating Layer fc8
I0223 11:39:00.902235 27291 net.cpp:408] fc8 <- fc7
I0223 11:39:00.902259 27291 net.cpp:382] fc8 -> fc8
I0223 11:39:00.966593 27291 net.cpp:124] Setting up fc8
I0223 11:39:00.966639 27291 net.cpp:131] Top shape: 256 1000 (256000)
I0223 11:39:00.966645 27291 net.cpp:139] Memory required for data: 1757220864
I0223 11:39:00.966660 27291 layer_factory.hpp:77] Creating layer loss
I0223 11:39:00.966675 27291 net.cpp:86] Creating Layer loss
I0223 11:39:00.966682 27291 net.cpp:408] loss <- fc8
I0223 11:39:00.966692 27291 net.cpp:408] loss <- label
I0223 11:39:00.966709 27291 net.cpp:382] loss -> loss
I0223 11:39:00.966733 27291 layer_factory.hpp:77] Creating layer loss
I0223 11:39:01.154531 27291 net.cpp:124] Setting up loss
I0223 11:39:01.154583 27291 net.cpp:131] Top shape: (1)
I0223 11:39:01.154592 27291 net.cpp:134]     with loss weight 1
I0223 11:39:01.154634 27291 net.cpp:139] Memory required for data: 1757220868
I0223 11:39:01.154645 27291 net.cpp:200] loss needs backward computation.
I0223 11:39:01.154661 27291 net.cpp:200] fc8 needs backward computation.
I0223 11:39:01.154670 27291 net.cpp:200] drop7 needs backward computation.
I0223 11:39:01.154677 27291 net.cpp:200] relu7 needs backward computation.
I0223 11:39:01.154685 27291 net.cpp:200] fc7 needs backward computation.
I0223 11:39:01.154692 27291 net.cpp:200] drop6 needs backward computation.
I0223 11:39:01.154700 27291 net.cpp:200] relu6 needs backward computation.
I0223 11:39:01.154706 27291 net.cpp:200] fc6 needs backward computation.
I0223 11:39:01.154714 27291 net.cpp:200] pool5 needs backward computation.
I0223 11:39:01.154722 27291 net.cpp:200] relu5 needs backward computation.
I0223 11:39:01.154729 27291 net.cpp:200] conv5 needs backward computation.
I0223 11:39:01.154736 27291 net.cpp:200] relu4 needs backward computation.
I0223 11:39:01.154744 27291 net.cpp:200] conv4 needs backward computation.
I0223 11:39:01.154752 27291 net.cpp:200] relu3 needs backward computation.
I0223 11:39:01.154759 27291 net.cpp:200] conv3 needs backward computation.
I0223 11:39:01.154767 27291 net.cpp:200] norm2 needs backward computation.
I0223 11:39:01.154775 27291 net.cpp:200] pool2 needs backward computation.
I0223 11:39:01.154783 27291 net.cpp:200] relu2 needs backward computation.
I0223 11:39:01.154790 27291 net.cpp:200] conv2 needs backward computation.
I0223 11:39:01.154798 27291 net.cpp:200] norm1 needs backward computation.
I0223 11:39:01.154831 27291 net.cpp:200] pool1 needs backward computation.
I0223 11:39:01.154840 27291 net.cpp:200] relu1 needs backward computation.
I0223 11:39:01.154847 27291 net.cpp:200] conv1 needs backward computation.
I0223 11:39:01.154856 27291 net.cpp:202] data does not need backward computation.
I0223 11:39:01.154863 27291 net.cpp:244] This network produces output loss
I0223 11:39:01.154896 27291 net.cpp:257] Network initialization done.
I0223 11:39:01.155397 27291 solver.cpp:173] Creating test net (#0) specified by net file: models/caffenet_proj/train_val.prototxt
I0223 11:39:01.155465 27291 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0223 11:39:01.155791 27291 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "examples/imagenet/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0223 11:39:01.156018 27291 layer_factory.hpp:77] Creating layer data
I0223 11:39:01.156128 27291 db_lmdb.cpp:35] Opened lmdb examples/imagenet/ilsvrc12_val_lmdb
I0223 11:39:01.156167 27291 net.cpp:86] Creating Layer data
I0223 11:39:01.156184 27291 net.cpp:382] data -> data
I0223 11:39:01.156204 27291 net.cpp:382] data -> label
I0223 11:39:01.156222 27291 data_transformer.cpp:25] Loading mean file from: data/ilsvrc12/imagenet_mean.binaryproto
I0223 11:39:01.179786 27291 data_layer.cpp:45] output data size: 50,3,227,227
I0223 11:39:01.265939 27291 net.cpp:124] Setting up data
I0223 11:39:01.265991 27291 net.cpp:131] Top shape: 50 3 227 227 (7729350)
I0223 11:39:01.266000 27291 net.cpp:131] Top shape: 50 (50)
I0223 11:39:01.266006 27291 net.cpp:139] Memory required for data: 30917600
I0223 11:39:01.266016 27291 layer_factory.hpp:77] Creating layer label_data_1_split
I0223 11:39:01.266034 27291 net.cpp:86] Creating Layer label_data_1_split
I0223 11:39:01.266042 27291 net.cpp:408] label_data_1_split <- label
I0223 11:39:01.266054 27291 net.cpp:382] label_data_1_split -> label_data_1_split_0
I0223 11:39:01.266070 27291 net.cpp:382] label_data_1_split -> label_data_1_split_1
I0223 11:39:01.266156 27291 net.cpp:124] Setting up label_data_1_split
I0223 11:39:01.266168 27291 net.cpp:131] Top shape: 50 (50)
I0223 11:39:01.266175 27291 net.cpp:131] Top shape: 50 (50)
I0223 11:39:01.266180 27291 net.cpp:139] Memory required for data: 30918000
I0223 11:39:01.266185 27291 layer_factory.hpp:77] Creating layer conv1
I0223 11:39:01.266204 27291 net.cpp:86] Creating Layer conv1
I0223 11:39:01.266211 27291 net.cpp:408] conv1 <- data
I0223 11:39:01.266222 27291 net.cpp:382] conv1 -> conv1
I0223 11:39:01.273448 27291 net.cpp:124] Setting up conv1
I0223 11:39:01.273494 27291 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0223 11:39:01.273501 27291 net.cpp:139] Memory required for data: 88998000
I0223 11:39:01.273522 27291 layer_factory.hpp:77] Creating layer relu1
I0223 11:39:01.273536 27291 net.cpp:86] Creating Layer relu1
I0223 11:39:01.273543 27291 net.cpp:408] relu1 <- conv1
I0223 11:39:01.273552 27291 net.cpp:369] relu1 -> conv1 (in-place)
I0223 11:39:01.273766 27291 net.cpp:124] Setting up relu1
I0223 11:39:01.273780 27291 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0223 11:39:01.273787 27291 net.cpp:139] Memory required for data: 147078000
I0223 11:39:01.273792 27291 layer_factory.hpp:77] Creating layer pool1
I0223 11:39:01.273805 27291 net.cpp:86] Creating Layer pool1
I0223 11:39:01.273813 27291 net.cpp:408] pool1 <- conv1
I0223 11:39:01.273821 27291 net.cpp:382] pool1 -> pool1
I0223 11:39:01.273880 27291 net.cpp:124] Setting up pool1
I0223 11:39:01.273892 27291 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0223 11:39:01.273897 27291 net.cpp:139] Memory required for data: 161074800
I0223 11:39:01.273903 27291 layer_factory.hpp:77] Creating layer norm1
I0223 11:39:01.273914 27291 net.cpp:86] Creating Layer norm1
I0223 11:39:01.273921 27291 net.cpp:408] norm1 <- pool1
I0223 11:39:01.273929 27291 net.cpp:382] norm1 -> norm1
I0223 11:39:01.274368 27291 net.cpp:124] Setting up norm1
I0223 11:39:01.274386 27291 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0223 11:39:01.274392 27291 net.cpp:139] Memory required for data: 175071600
I0223 11:39:01.274425 27291 layer_factory.hpp:77] Creating layer conv2
I0223 11:39:01.274446 27291 net.cpp:86] Creating Layer conv2
I0223 11:39:01.274452 27291 net.cpp:408] conv2 <- norm1
I0223 11:39:01.274463 27291 net.cpp:382] conv2 -> conv2
I0223 11:39:01.282747 27291 net.cpp:124] Setting up conv2
I0223 11:39:01.282799 27291 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0223 11:39:01.282804 27291 net.cpp:139] Memory required for data: 212396400
I0223 11:39:01.282824 27291 layer_factory.hpp:77] Creating layer relu2
I0223 11:39:01.282840 27291 net.cpp:86] Creating Layer relu2
I0223 11:39:01.282847 27291 net.cpp:408] relu2 <- conv2
I0223 11:39:01.282858 27291 net.cpp:369] relu2 -> conv2 (in-place)
I0223 11:39:01.283076 27291 net.cpp:124] Setting up relu2
I0223 11:39:01.283102 27291 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0223 11:39:01.283123 27291 net.cpp:139] Memory required for data: 249721200
I0223 11:39:01.283128 27291 layer_factory.hpp:77] Creating layer pool2
I0223 11:39:01.283143 27291 net.cpp:86] Creating Layer pool2
I0223 11:39:01.283149 27291 net.cpp:408] pool2 <- conv2
I0223 11:39:01.283157 27291 net.cpp:382] pool2 -> pool2
I0223 11:39:01.283218 27291 net.cpp:124] Setting up pool2
I0223 11:39:01.283231 27291 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0223 11:39:01.283236 27291 net.cpp:139] Memory required for data: 258374000
I0223 11:39:01.283241 27291 layer_factory.hpp:77] Creating layer norm2
I0223 11:39:01.283253 27291 net.cpp:86] Creating Layer norm2
I0223 11:39:01.283259 27291 net.cpp:408] norm2 <- pool2
I0223 11:39:01.283268 27291 net.cpp:382] norm2 -> norm2
I0223 11:39:01.283702 27291 net.cpp:124] Setting up norm2
I0223 11:39:01.283718 27291 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0223 11:39:01.283725 27291 net.cpp:139] Memory required for data: 267026800
I0223 11:39:01.283730 27291 layer_factory.hpp:77] Creating layer conv3
I0223 11:39:01.283747 27291 net.cpp:86] Creating Layer conv3
I0223 11:39:01.283754 27291 net.cpp:408] conv3 <- norm2
I0223 11:39:01.283764 27291 net.cpp:382] conv3 -> conv3
I0223 11:39:01.299298 27291 net.cpp:124] Setting up conv3
I0223 11:39:01.299346 27291 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0223 11:39:01.299353 27291 net.cpp:139] Memory required for data: 280006000
I0223 11:39:01.299374 27291 layer_factory.hpp:77] Creating layer relu3
I0223 11:39:01.299388 27291 net.cpp:86] Creating Layer relu3
I0223 11:39:01.299396 27291 net.cpp:408] relu3 <- conv3
I0223 11:39:01.299407 27291 net.cpp:369] relu3 -> conv3 (in-place)
I0223 11:39:01.299808 27291 net.cpp:124] Setting up relu3
I0223 11:39:01.299823 27291 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0223 11:39:01.299829 27291 net.cpp:139] Memory required for data: 292985200
I0223 11:39:01.299835 27291 layer_factory.hpp:77] Creating layer conv4
I0223 11:39:01.299851 27291 net.cpp:86] Creating Layer conv4
I0223 11:39:01.299859 27291 net.cpp:408] conv4 <- conv3
I0223 11:39:01.299868 27291 net.cpp:382] conv4 -> conv4
I0223 11:39:01.322974 27291 net.cpp:124] Setting up conv4
I0223 11:39:01.323029 27291 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0223 11:39:01.323035 27291 net.cpp:139] Memory required for data: 305964400
I0223 11:39:01.323051 27291 layer_factory.hpp:77] Creating layer relu4
I0223 11:39:01.323066 27291 net.cpp:86] Creating Layer relu4
I0223 11:39:01.323074 27291 net.cpp:408] relu4 <- conv4
I0223 11:39:01.323086 27291 net.cpp:369] relu4 -> conv4 (in-place)
I0223 11:39:01.323320 27291 net.cpp:124] Setting up relu4
I0223 11:39:01.323334 27291 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0223 11:39:01.323340 27291 net.cpp:139] Memory required for data: 318943600
I0223 11:39:01.323348 27291 layer_factory.hpp:77] Creating layer conv5
I0223 11:39:01.323365 27291 net.cpp:86] Creating Layer conv5
I0223 11:39:01.323372 27291 net.cpp:408] conv5 <- conv4
I0223 11:39:01.323382 27291 net.cpp:382] conv5 -> conv5
I0223 11:39:01.333160 27291 net.cpp:124] Setting up conv5
I0223 11:39:01.333210 27291 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0223 11:39:01.333216 27291 net.cpp:139] Memory required for data: 327596400
I0223 11:39:01.333240 27291 layer_factory.hpp:77] Creating layer relu5
I0223 11:39:01.333256 27291 net.cpp:86] Creating Layer relu5
I0223 11:39:01.333263 27291 net.cpp:408] relu5 <- conv5
I0223 11:39:01.333276 27291 net.cpp:369] relu5 -> conv5 (in-place)
I0223 11:39:01.333497 27291 net.cpp:124] Setting up relu5
I0223 11:39:01.333511 27291 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0223 11:39:01.333518 27291 net.cpp:139] Memory required for data: 336249200
I0223 11:39:01.333523 27291 layer_factory.hpp:77] Creating layer pool5
I0223 11:39:01.333539 27291 net.cpp:86] Creating Layer pool5
I0223 11:39:01.333544 27291 net.cpp:408] pool5 <- conv5
I0223 11:39:01.333554 27291 net.cpp:382] pool5 -> pool5
I0223 11:39:01.333629 27291 net.cpp:124] Setting up pool5
I0223 11:39:01.333654 27291 net.cpp:131] Top shape: 50 256 6 6 (460800)
I0223 11:39:01.333660 27291 net.cpp:139] Memory required for data: 338092400
I0223 11:39:01.333667 27291 layer_factory.hpp:77] Creating layer fc6
I0223 11:39:01.333679 27291 net.cpp:86] Creating Layer fc6
I0223 11:39:01.333685 27291 net.cpp:408] fc6 <- pool5
I0223 11:39:01.333694 27291 net.cpp:382] fc6 -> fc6
I0223 11:39:01.925012 27291 net.cpp:124] Setting up fc6
I0223 11:39:01.925066 27291 net.cpp:131] Top shape: 50 4096 (204800)
I0223 11:39:01.925072 27291 net.cpp:139] Memory required for data: 338911600
I0223 11:39:01.925087 27291 layer_factory.hpp:77] Creating layer relu6
I0223 11:39:01.925102 27291 net.cpp:86] Creating Layer relu6
I0223 11:39:01.925109 27291 net.cpp:408] relu6 <- fc6
I0223 11:39:01.925120 27291 net.cpp:369] relu6 -> fc6 (in-place)
I0223 11:39:01.925688 27291 net.cpp:124] Setting up relu6
I0223 11:39:01.925704 27291 net.cpp:131] Top shape: 50 4096 (204800)
I0223 11:39:01.925710 27291 net.cpp:139] Memory required for data: 339730800
I0223 11:39:01.925717 27291 layer_factory.hpp:77] Creating layer drop6
I0223 11:39:01.925729 27291 net.cpp:86] Creating Layer drop6
I0223 11:39:01.925734 27291 net.cpp:408] drop6 <- fc6
I0223 11:39:01.925742 27291 net.cpp:369] drop6 -> fc6 (in-place)
I0223 11:39:01.925786 27291 net.cpp:124] Setting up drop6
I0223 11:39:01.925796 27291 net.cpp:131] Top shape: 50 4096 (204800)
I0223 11:39:01.925802 27291 net.cpp:139] Memory required for data: 340550000
I0223 11:39:01.925807 27291 layer_factory.hpp:77] Creating layer fc7
I0223 11:39:01.925819 27291 net.cpp:86] Creating Layer fc7
I0223 11:39:01.925825 27291 net.cpp:408] fc7 <- fc6
I0223 11:39:01.925834 27291 net.cpp:382] fc7 -> fc7
I0223 11:39:02.187759 27291 net.cpp:124] Setting up fc7
I0223 11:39:02.187809 27291 net.cpp:131] Top shape: 50 4096 (204800)
I0223 11:39:02.187815 27291 net.cpp:139] Memory required for data: 341369200
I0223 11:39:02.187830 27291 layer_factory.hpp:77] Creating layer relu7
I0223 11:39:02.187844 27291 net.cpp:86] Creating Layer relu7
I0223 11:39:02.187851 27291 net.cpp:408] relu7 <- fc7
I0223 11:39:02.187862 27291 net.cpp:369] relu7 -> fc7 (in-place)
I0223 11:39:02.188153 27291 net.cpp:124] Setting up relu7
I0223 11:39:02.188166 27291 net.cpp:131] Top shape: 50 4096 (204800)
I0223 11:39:02.188172 27291 net.cpp:139] Memory required for data: 342188400
I0223 11:39:02.188179 27291 layer_factory.hpp:77] Creating layer drop7
I0223 11:39:02.188189 27291 net.cpp:86] Creating Layer drop7
I0223 11:39:02.188195 27291 net.cpp:408] drop7 <- fc7
I0223 11:39:02.188204 27291 net.cpp:369] drop7 -> fc7 (in-place)
I0223 11:39:02.188246 27291 net.cpp:124] Setting up drop7
I0223 11:39:02.188257 27291 net.cpp:131] Top shape: 50 4096 (204800)
I0223 11:39:02.188262 27291 net.cpp:139] Memory required for data: 343007600
I0223 11:39:02.188268 27291 layer_factory.hpp:77] Creating layer fc8
I0223 11:39:02.188280 27291 net.cpp:86] Creating Layer fc8
I0223 11:39:02.188287 27291 net.cpp:408] fc8 <- fc7
I0223 11:39:02.188295 27291 net.cpp:382] fc8 -> fc8
I0223 11:39:02.407955 27291 net.cpp:124] Setting up fc8
I0223 11:39:02.408015 27291 net.cpp:131] Top shape: 50 1000 (50000)
I0223 11:39:02.408020 27291 net.cpp:139] Memory required for data: 343207600
I0223 11:39:02.408037 27291 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0223 11:39:02.408052 27291 net.cpp:86] Creating Layer fc8_fc8_0_split
I0223 11:39:02.408061 27291 net.cpp:408] fc8_fc8_0_split <- fc8
I0223 11:39:02.408073 27291 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0223 11:39:02.408090 27291 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0223 11:39:02.408149 27291 net.cpp:124] Setting up fc8_fc8_0_split
I0223 11:39:02.408159 27291 net.cpp:131] Top shape: 50 1000 (50000)
I0223 11:39:02.408166 27291 net.cpp:131] Top shape: 50 1000 (50000)
I0223 11:39:02.408171 27291 net.cpp:139] Memory required for data: 343607600
I0223 11:39:02.408177 27291 layer_factory.hpp:77] Creating layer accuracy
I0223 11:39:02.408188 27291 net.cpp:86] Creating Layer accuracy
I0223 11:39:02.408210 27291 net.cpp:408] accuracy <- fc8_fc8_0_split_0
I0223 11:39:02.408231 27291 net.cpp:408] accuracy <- label_data_1_split_0
I0223 11:39:02.408241 27291 net.cpp:382] accuracy -> accuracy
I0223 11:39:02.408254 27291 net.cpp:124] Setting up accuracy
I0223 11:39:02.408262 27291 net.cpp:131] Top shape: (1)
I0223 11:39:02.408267 27291 net.cpp:139] Memory required for data: 343607604
I0223 11:39:02.408273 27291 layer_factory.hpp:77] Creating layer loss
I0223 11:39:02.408282 27291 net.cpp:86] Creating Layer loss
I0223 11:39:02.408288 27291 net.cpp:408] loss <- fc8_fc8_0_split_1
I0223 11:39:02.408295 27291 net.cpp:408] loss <- label_data_1_split_1
I0223 11:39:02.408303 27291 net.cpp:382] loss -> loss
I0223 11:39:02.408315 27291 layer_factory.hpp:77] Creating layer loss
I0223 11:39:02.409062 27291 net.cpp:124] Setting up loss
I0223 11:39:02.409080 27291 net.cpp:131] Top shape: (1)
I0223 11:39:02.409085 27291 net.cpp:134]     with loss weight 1
I0223 11:39:02.409104 27291 net.cpp:139] Memory required for data: 343607608
I0223 11:39:02.409111 27291 net.cpp:200] loss needs backward computation.
I0223 11:39:02.409119 27291 net.cpp:202] accuracy does not need backward computation.
I0223 11:39:02.409126 27291 net.cpp:200] fc8_fc8_0_split needs backward computation.
I0223 11:39:02.409132 27291 net.cpp:200] fc8 needs backward computation.
I0223 11:39:02.409138 27291 net.cpp:200] drop7 needs backward computation.
I0223 11:39:02.409144 27291 net.cpp:200] relu7 needs backward computation.
I0223 11:39:02.409150 27291 net.cpp:200] fc7 needs backward computation.
I0223 11:39:02.409155 27291 net.cpp:200] drop6 needs backward computation.
I0223 11:39:02.409162 27291 net.cpp:200] relu6 needs backward computation.
I0223 11:39:02.409167 27291 net.cpp:200] fc6 needs backward computation.
I0223 11:39:02.409173 27291 net.cpp:200] pool5 needs backward computation.
I0223 11:39:02.409178 27291 net.cpp:200] relu5 needs backward computation.
I0223 11:39:02.409184 27291 net.cpp:200] conv5 needs backward computation.
I0223 11:39:02.409190 27291 net.cpp:200] relu4 needs backward computation.
I0223 11:39:02.409195 27291 net.cpp:200] conv4 needs backward computation.
I0223 11:39:02.409201 27291 net.cpp:200] relu3 needs backward computation.
I0223 11:39:02.409207 27291 net.cpp:200] conv3 needs backward computation.
I0223 11:39:02.409214 27291 net.cpp:200] norm2 needs backward computation.
I0223 11:39:02.409219 27291 net.cpp:200] pool2 needs backward computation.
I0223 11:39:02.409224 27291 net.cpp:200] relu2 needs backward computation.
I0223 11:39:02.409230 27291 net.cpp:200] conv2 needs backward computation.
I0223 11:39:02.409236 27291 net.cpp:200] norm1 needs backward computation.
I0223 11:39:02.409242 27291 net.cpp:200] pool1 needs backward computation.
I0223 11:39:02.409248 27291 net.cpp:200] relu1 needs backward computation.
I0223 11:39:02.409253 27291 net.cpp:200] conv1 needs backward computation.
I0223 11:39:02.409260 27291 net.cpp:202] label_data_1_split does not need backward computation.
I0223 11:39:02.409267 27291 net.cpp:202] data does not need backward computation.
I0223 11:39:02.409272 27291 net.cpp:244] This network produces output accuracy
I0223 11:39:02.409278 27291 net.cpp:244] This network produces output loss
I0223 11:39:02.409301 27291 net.cpp:257] Network initialization done.
I0223 11:39:02.409400 27291 solver.cpp:56] Solver scaffolding done.
I0223 11:39:02.410151 27291 caffe.cpp:248] Starting Optimization
I0223 11:39:02.410161 27291 solver.cpp:273] Solving CaffeNet
I0223 11:39:02.410167 27291 solver.cpp:274] Learning Rate Policy: fixed
I0223 11:39:02.412559 27291 solver.cpp:331] Iteration 0, Testing net (#0)
I0223 11:39:09.415859 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0012
I0223 11:39:09.415935 27291 solver.cpp:398]     Test net output #1: loss = 7.12576 (* 1 = 7.12576 loss)
I0223 11:39:10.389286 27291 solver.cpp:219] Iteration 0 (0 iter/s, 7.97871s/20 iters), loss = 7.3978
I0223 11:39:10.389345 27291 solver.cpp:238]     Train net output #0: loss = 7.3978 (* 1 = 7.3978 loss)
I0223 11:39:10.389369 27291 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0223 11:39:30.412776 27291 solver.cpp:219] Iteration 20 (0.998848 iter/s, 20.0231s/20 iters), loss = 7.08147
I0223 11:39:30.436393 27291 solver.cpp:238]     Train net output #0: loss = 7.08147 (* 1 = 7.08147 loss)
I0223 11:39:30.436424 27291 sgd_solver.cpp:105] Iteration 20, lr = 0.01
I0223 11:39:46.759423 27291 blocking_queue.cpp:49] Waiting for data
I0223 11:39:50.383605 27291 solver.cpp:219] Iteration 40 (1.00266 iter/s, 19.9469s/20 iters), loss = 6.95854
I0223 11:39:50.407227 27291 solver.cpp:238]     Train net output #0: loss = 6.95854 (* 1 = 6.95854 loss)
I0223 11:39:50.407258 27291 sgd_solver.cpp:105] Iteration 40, lr = 0.01
I0223 11:40:10.114019 27291 solver.cpp:219] Iteration 60 (1.0149 iter/s, 19.7064s/20 iters), loss = 6.87132
I0223 11:40:10.137653 27291 solver.cpp:238]     Train net output #0: loss = 6.87132 (* 1 = 6.87132 loss)
I0223 11:40:10.137686 27291 sgd_solver.cpp:105] Iteration 60, lr = 0.01
I0223 11:40:29.836700 27291 solver.cpp:219] Iteration 80 (1.0153 iter/s, 19.6987s/20 iters), loss = 6.91947
I0223 11:40:29.860311 27291 solver.cpp:238]     Train net output #0: loss = 6.91947 (* 1 = 6.91947 loss)
I0223 11:40:29.860344 27291 sgd_solver.cpp:105] Iteration 80, lr = 0.01
I0223 11:40:47.914046 27291 solver.cpp:331] Iteration 100, Testing net (#0)
I0223 11:41:13.130164 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0006
I0223 11:41:13.130234 27291 solver.cpp:398]     Test net output #1: loss = 6.94677 (* 1 = 6.94677 loss)
I0223 11:41:14.096047 27291 solver.cpp:219] Iteration 100 (0.452132 iter/s, 44.2349s/20 iters), loss = 6.89895
I0223 11:41:14.096135 27291 solver.cpp:238]     Train net output #0: loss = 6.89895 (* 1 = 6.89895 loss)
I0223 11:41:14.096149 27291 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I0223 11:41:33.844977 27291 solver.cpp:219] Iteration 120 (1.01274 iter/s, 19.7484s/20 iters), loss = 6.90071
I0223 11:41:33.868613 27291 solver.cpp:238]     Train net output #0: loss = 6.90071 (* 1 = 6.90071 loss)
I0223 11:41:33.868643 27291 sgd_solver.cpp:105] Iteration 120, lr = 0.01
I0223 11:41:53.580766 27291 solver.cpp:219] Iteration 140 (1.01462 iter/s, 19.7117s/20 iters), loss = 6.88067
I0223 11:41:53.604378 27291 solver.cpp:238]     Train net output #0: loss = 6.88067 (* 1 = 6.88067 loss)
I0223 11:41:53.604409 27291 sgd_solver.cpp:105] Iteration 140, lr = 0.01
I0223 11:42:13.302044 27291 solver.cpp:219] Iteration 160 (1.01537 iter/s, 19.6973s/20 iters), loss = 6.88172
I0223 11:42:13.325669 27291 solver.cpp:238]     Train net output #0: loss = 6.88172 (* 1 = 6.88172 loss)
I0223 11:42:13.325700 27291 sgd_solver.cpp:105] Iteration 160, lr = 0.01
I0223 11:42:33.006880 27291 solver.cpp:219] Iteration 180 (1.01622 iter/s, 19.6808s/20 iters), loss = 6.90616
I0223 11:42:33.030501 27291 solver.cpp:238]     Train net output #0: loss = 6.90616 (* 1 = 6.90616 loss)
I0223 11:42:33.030534 27291 sgd_solver.cpp:105] Iteration 180, lr = 0.01
I0223 11:42:51.077255 27291 solver.cpp:331] Iteration 200, Testing net (#0)
I0223 11:43:08.483974 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0016
I0223 11:43:08.484055 27291 solver.cpp:398]     Test net output #1: loss = 6.94714 (* 1 = 6.94714 loss)
I0223 11:43:09.454530 27291 solver.cpp:219] Iteration 200 (0.549097 iter/s, 36.4235s/20 iters), loss = 6.87837
I0223 11:43:09.454615 27291 solver.cpp:238]     Train net output #0: loss = 6.87837 (* 1 = 6.87837 loss)
I0223 11:43:09.454629 27291 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I0223 11:43:29.173977 27291 solver.cpp:219] Iteration 220 (1.01425 iter/s, 19.719s/20 iters), loss = 6.85933
I0223 11:43:29.197592 27291 solver.cpp:238]     Train net output #0: loss = 6.85933 (* 1 = 6.85933 loss)
I0223 11:43:29.197623 27291 sgd_solver.cpp:105] Iteration 220, lr = 0.01
I0223 11:43:48.899055 27291 solver.cpp:219] Iteration 240 (1.01517 iter/s, 19.7012s/20 iters), loss = 6.8645
I0223 11:43:48.922677 27291 solver.cpp:238]     Train net output #0: loss = 6.8645 (* 1 = 6.8645 loss)
I0223 11:43:48.922713 27291 sgd_solver.cpp:105] Iteration 240, lr = 0.01
I0223 11:44:08.977803 27291 solver.cpp:219] Iteration 260 (0.997267 iter/s, 20.0548s/20 iters), loss = 6.86847
I0223 11:44:09.001420 27291 solver.cpp:238]     Train net output #0: loss = 6.86847 (* 1 = 6.86847 loss)
I0223 11:44:09.001451 27291 sgd_solver.cpp:105] Iteration 260, lr = 0.01
I0223 11:44:28.994279 27291 solver.cpp:219] Iteration 280 (1.00037 iter/s, 19.9925s/20 iters), loss = 6.87357
I0223 11:44:29.017896 27291 solver.cpp:238]     Train net output #0: loss = 6.87357 (* 1 = 6.87357 loss)
I0223 11:44:29.017930 27291 sgd_solver.cpp:105] Iteration 280, lr = 0.01
I0223 11:44:47.073241 27291 solver.cpp:331] Iteration 300, Testing net (#0)
I0223 11:45:15.724339 27291 solver.cpp:398]     Test net output #0: accuracy = 0.002
I0223 11:45:15.724437 27291 solver.cpp:398]     Test net output #1: loss = 6.96511 (* 1 = 6.96511 loss)
I0223 11:45:16.690383 27291 solver.cpp:219] Iteration 300 (0.419536 iter/s, 47.6717s/20 iters), loss = 6.84686
I0223 11:45:16.695097 27291 solver.cpp:238]     Train net output #0: loss = 6.84686 (* 1 = 6.84686 loss)
I0223 11:45:16.695135 27291 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I0223 11:45:36.437515 27291 solver.cpp:219] Iteration 320 (1.01306 iter/s, 19.7421s/20 iters), loss = 6.85842
I0223 11:45:36.461127 27291 solver.cpp:238]     Train net output #0: loss = 6.85842 (* 1 = 6.85842 loss)
I0223 11:45:36.461155 27291 sgd_solver.cpp:105] Iteration 320, lr = 0.01
I0223 11:45:56.159205 27291 solver.cpp:219] Iteration 340 (1.01534 iter/s, 19.6978s/20 iters), loss = 6.87591
I0223 11:45:56.182832 27291 solver.cpp:238]     Train net output #0: loss = 6.87591 (* 1 = 6.87591 loss)
I0223 11:45:56.182867 27291 sgd_solver.cpp:105] Iteration 340, lr = 0.01
I0223 11:46:15.912263 27291 solver.cpp:219] Iteration 360 (1.01373 iter/s, 19.7291s/20 iters), loss = 6.8678
I0223 11:46:15.935878 27291 solver.cpp:238]     Train net output #0: loss = 6.8678 (* 1 = 6.8678 loss)
I0223 11:46:15.935909 27291 sgd_solver.cpp:105] Iteration 360, lr = 0.01
I0223 11:46:35.809586 27291 solver.cpp:219] Iteration 380 (1.00637 iter/s, 19.8734s/20 iters), loss = 6.85811
I0223 11:46:35.833206 27291 solver.cpp:238]     Train net output #0: loss = 6.85811 (* 1 = 6.85811 loss)
I0223 11:46:35.833237 27291 sgd_solver.cpp:105] Iteration 380, lr = 0.01
I0223 11:46:53.870584 27291 solver.cpp:331] Iteration 400, Testing net (#0)
I0223 11:47:17.904696 27291 solver.cpp:398]     Test net output #0: accuracy = 0.001
I0223 11:47:17.904773 27291 solver.cpp:398]     Test net output #1: loss = 6.94862 (* 1 = 6.94862 loss)
I0223 11:47:18.873106 27291 solver.cpp:219] Iteration 400 (0.464693 iter/s, 43.0392s/20 iters), loss = 6.85738
I0223 11:47:18.873193 27291 solver.cpp:238]     Train net output #0: loss = 6.85738 (* 1 = 6.85738 loss)
I0223 11:47:18.873208 27291 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I0223 11:47:38.735780 27291 solver.cpp:219] Iteration 420 (1.00694 iter/s, 19.8623s/20 iters), loss = 6.8248
I0223 11:47:38.759393 27291 solver.cpp:238]     Train net output #0: loss = 6.8248 (* 1 = 6.8248 loss)
I0223 11:47:38.759423 27291 sgd_solver.cpp:105] Iteration 420, lr = 0.01
I0223 11:47:58.468993 27291 solver.cpp:219] Iteration 440 (1.01475 iter/s, 19.7093s/20 iters), loss = 6.81397
I0223 11:47:58.492612 27291 solver.cpp:238]     Train net output #0: loss = 6.81397 (* 1 = 6.81397 loss)
I0223 11:47:58.492645 27291 sgd_solver.cpp:105] Iteration 440, lr = 0.01
I0223 11:48:18.193172 27291 solver.cpp:219] Iteration 460 (1.01522 iter/s, 19.7002s/20 iters), loss = 6.80617
I0223 11:48:18.216789 27291 solver.cpp:238]     Train net output #0: loss = 6.80617 (* 1 = 6.80617 loss)
I0223 11:48:18.216819 27291 sgd_solver.cpp:105] Iteration 460, lr = 0.01
I0223 11:48:37.934815 27291 solver.cpp:219] Iteration 480 (1.01432 iter/s, 19.7177s/20 iters), loss = 6.75466
I0223 11:48:37.958444 27291 solver.cpp:238]     Train net output #0: loss = 6.75466 (* 1 = 6.75466 loss)
I0223 11:48:37.958475 27291 sgd_solver.cpp:105] Iteration 480, lr = 0.01
I0223 11:48:56.026015 27291 solver.cpp:331] Iteration 500, Testing net (#0)
I0223 11:49:13.836891 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0022
I0223 11:49:13.836993 27291 solver.cpp:398]     Test net output #1: loss = 6.90581 (* 1 = 6.90581 loss)
I0223 11:49:14.803305 27291 solver.cpp:219] Iteration 500 (0.542826 iter/s, 36.8442s/20 iters), loss = 6.76122
I0223 11:49:14.803387 27291 solver.cpp:238]     Train net output #0: loss = 6.76122 (* 1 = 6.76122 loss)
I0223 11:49:14.803400 27291 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I0223 11:49:34.539968 27291 solver.cpp:219] Iteration 520 (1.01336 iter/s, 19.7362s/20 iters), loss = 6.74887
I0223 11:49:34.563590 27291 solver.cpp:238]     Train net output #0: loss = 6.74887 (* 1 = 6.74887 loss)
I0223 11:49:34.563619 27291 sgd_solver.cpp:105] Iteration 520, lr = 0.01
I0223 11:49:54.281812 27291 solver.cpp:219] Iteration 540 (1.01431 iter/s, 19.7179s/20 iters), loss = 6.8307
I0223 11:49:54.305428 27291 solver.cpp:238]     Train net output #0: loss = 6.8307 (* 1 = 6.8307 loss)
I0223 11:49:54.305459 27291 sgd_solver.cpp:105] Iteration 540, lr = 0.01
I0223 11:50:14.018081 27291 solver.cpp:219] Iteration 560 (1.01459 iter/s, 19.7123s/20 iters), loss = 6.79479
I0223 11:50:14.041692 27291 solver.cpp:238]     Train net output #0: loss = 6.79479 (* 1 = 6.79479 loss)
I0223 11:50:14.041723 27291 sgd_solver.cpp:105] Iteration 560, lr = 0.01
I0223 11:50:33.732823 27291 solver.cpp:219] Iteration 580 (1.0157 iter/s, 19.6908s/20 iters), loss = 6.76417
I0223 11:50:33.756436 27291 solver.cpp:238]     Train net output #0: loss = 6.76417 (* 1 = 6.76417 loss)
I0223 11:50:33.756467 27291 sgd_solver.cpp:105] Iteration 580, lr = 0.01
I0223 11:50:51.834231 27291 solver.cpp:331] Iteration 600, Testing net (#0)
I0223 11:51:05.571539 27291 blocking_queue.cpp:49] Waiting for data
I0223 11:51:07.395138 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0012
I0223 11:51:07.395241 27291 solver.cpp:398]     Test net output #1: loss = 6.88168 (* 1 = 6.88168 loss)
I0223 11:51:08.364848 27291 solver.cpp:219] Iteration 600 (0.577904 iter/s, 34.6078s/20 iters), loss = 6.75202
I0223 11:51:08.364930 27291 solver.cpp:238]     Train net output #0: loss = 6.75202 (* 1 = 6.75202 loss)
I0223 11:51:08.364943 27291 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I0223 11:51:28.176000 27291 solver.cpp:219] Iteration 620 (1.00955 iter/s, 19.8107s/20 iters), loss = 6.79265
I0223 11:51:28.199617 27291 solver.cpp:238]     Train net output #0: loss = 6.79265 (* 1 = 6.79265 loss)
I0223 11:51:28.199648 27291 sgd_solver.cpp:105] Iteration 620, lr = 0.01
I0223 11:51:47.979472 27291 solver.cpp:219] Iteration 640 (1.01115 iter/s, 19.7795s/20 iters), loss = 6.68102
I0223 11:51:48.003095 27291 solver.cpp:238]     Train net output #0: loss = 6.68102 (* 1 = 6.68102 loss)
I0223 11:51:48.003126 27291 sgd_solver.cpp:105] Iteration 640, lr = 0.01
I0223 11:52:07.718047 27291 solver.cpp:219] Iteration 660 (1.01448 iter/s, 19.7146s/20 iters), loss = 6.70654
I0223 11:52:07.741658 27291 solver.cpp:238]     Train net output #0: loss = 6.70654 (* 1 = 6.70654 loss)
I0223 11:52:07.741688 27291 sgd_solver.cpp:105] Iteration 660, lr = 0.01
I0223 11:52:27.462990 27291 solver.cpp:219] Iteration 680 (1.01415 iter/s, 19.721s/20 iters), loss = 6.70104
I0223 11:52:27.486606 27291 solver.cpp:238]     Train net output #0: loss = 6.70104 (* 1 = 6.70104 loss)
I0223 11:52:27.486640 27291 sgd_solver.cpp:105] Iteration 680, lr = 0.01
I0223 11:52:45.695230 27291 solver.cpp:331] Iteration 700, Testing net (#0)
I0223 11:53:01.997845 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0022
I0223 11:53:01.997917 27291 solver.cpp:398]     Test net output #1: loss = 6.85562 (* 1 = 6.85562 loss)
I0223 11:53:02.963683 27291 solver.cpp:219] Iteration 700 (0.563754 iter/s, 35.4765s/20 iters), loss = 6.76276
I0223 11:53:02.968385 27291 solver.cpp:238]     Train net output #0: loss = 6.76276 (* 1 = 6.76276 loss)
I0223 11:53:02.968423 27291 sgd_solver.cpp:105] Iteration 700, lr = 0.01
I0223 11:53:22.669711 27291 solver.cpp:219] Iteration 720 (1.01518 iter/s, 19.701s/20 iters), loss = 6.71684
I0223 11:53:22.693330 27291 solver.cpp:238]     Train net output #0: loss = 6.71684 (* 1 = 6.71684 loss)
I0223 11:53:22.693369 27291 sgd_solver.cpp:105] Iteration 720, lr = 0.01
I0223 11:53:42.459776 27291 solver.cpp:219] Iteration 740 (1.01183 iter/s, 19.7661s/20 iters), loss = 6.76734
I0223 11:53:42.483393 27291 solver.cpp:238]     Train net output #0: loss = 6.76734 (* 1 = 6.76734 loss)
I0223 11:53:42.483427 27291 sgd_solver.cpp:105] Iteration 740, lr = 0.01
I0223 11:54:02.365967 27291 solver.cpp:219] Iteration 760 (1.00592 iter/s, 19.8822s/20 iters), loss = 6.71625
I0223 11:54:02.389582 27291 solver.cpp:238]     Train net output #0: loss = 6.71625 (* 1 = 6.71625 loss)
I0223 11:54:02.389611 27291 sgd_solver.cpp:105] Iteration 760, lr = 0.01
I0223 11:54:22.198817 27291 solver.cpp:219] Iteration 780 (1.00965 iter/s, 19.8089s/20 iters), loss = 6.65212
I0223 11:54:22.222446 27291 solver.cpp:238]     Train net output #0: loss = 6.65212 (* 1 = 6.65212 loss)
I0223 11:54:22.222478 27291 sgd_solver.cpp:105] Iteration 780, lr = 0.01
I0223 11:54:40.324930 27291 solver.cpp:331] Iteration 800, Testing net (#0)
I0223 11:54:59.233213 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0022
I0223 11:54:59.233314 27291 solver.cpp:398]     Test net output #1: loss = 6.83917 (* 1 = 6.83917 loss)
I0223 11:55:00.202689 27291 solver.cpp:219] Iteration 800 (0.526599 iter/s, 37.9796s/20 iters), loss = 6.73231
I0223 11:55:00.202776 27291 solver.cpp:238]     Train net output #0: loss = 6.73231 (* 1 = 6.73231 loss)
I0223 11:55:00.202790 27291 sgd_solver.cpp:105] Iteration 800, lr = 0.01
I0223 11:55:20.133872 27291 solver.cpp:219] Iteration 820 (1.00347 iter/s, 19.9307s/20 iters), loss = 6.70517
I0223 11:55:20.134114 27291 solver.cpp:238]     Train net output #0: loss = 6.70517 (* 1 = 6.70517 loss)
I0223 11:55:20.134130 27291 sgd_solver.cpp:105] Iteration 820, lr = 0.01
I0223 11:55:40.024054 27291 solver.cpp:219] Iteration 840 (1.00557 iter/s, 19.8892s/20 iters), loss = 6.63588
I0223 11:55:40.047667 27291 solver.cpp:238]     Train net output #0: loss = 6.63588 (* 1 = 6.63588 loss)
I0223 11:55:40.047696 27291 sgd_solver.cpp:105] Iteration 840, lr = 0.01
I0223 11:55:59.933912 27291 solver.cpp:219] Iteration 860 (1.00574 iter/s, 19.8859s/20 iters), loss = 6.60654
I0223 11:55:59.957530 27291 solver.cpp:238]     Train net output #0: loss = 6.60654 (* 1 = 6.60654 loss)
I0223 11:55:59.957561 27291 sgd_solver.cpp:105] Iteration 860, lr = 0.01
I0223 11:56:19.674027 27291 solver.cpp:219] Iteration 880 (1.0144 iter/s, 19.7162s/20 iters), loss = 6.67293
I0223 11:56:19.697641 27291 solver.cpp:238]     Train net output #0: loss = 6.67293 (* 1 = 6.67293 loss)
I0223 11:56:19.697674 27291 sgd_solver.cpp:105] Iteration 880, lr = 0.01
I0223 11:56:37.848906 27291 solver.cpp:331] Iteration 900, Testing net (#0)
I0223 11:56:58.590250 27318 data_layer.cpp:73] Restarting data prefetching from start.
I0223 11:56:58.670217 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0036
I0223 11:56:58.670290 27291 solver.cpp:398]     Test net output #1: loss = 6.80109 (* 1 = 6.80109 loss)
I0223 11:56:59.637441 27291 solver.cpp:219] Iteration 900 (0.500757 iter/s, 39.9395s/20 iters), loss = 6.68152
I0223 11:56:59.637521 27291 solver.cpp:238]     Train net output #0: loss = 6.68152 (* 1 = 6.68152 loss)
I0223 11:56:59.637536 27291 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I0223 11:57:19.674789 27291 solver.cpp:219] Iteration 920 (0.998146 iter/s, 20.0372s/20 iters), loss = 6.58759
I0223 11:57:19.674929 27291 solver.cpp:238]     Train net output #0: loss = 6.58759 (* 1 = 6.58759 loss)
I0223 11:57:19.674944 27291 sgd_solver.cpp:105] Iteration 920, lr = 0.01
I0223 11:57:39.543746 27291 solver.cpp:219] Iteration 940 (1.00661 iter/s, 19.8687s/20 iters), loss = 6.59592
I0223 11:57:39.567368 27291 solver.cpp:238]     Train net output #0: loss = 6.59592 (* 1 = 6.59592 loss)
I0223 11:57:39.567400 27291 sgd_solver.cpp:105] Iteration 940, lr = 0.01
I0223 11:57:59.282508 27291 solver.cpp:219] Iteration 960 (1.01445 iter/s, 19.715s/20 iters), loss = 6.59615
I0223 11:57:59.313005 27291 solver.cpp:238]     Train net output #0: loss = 6.59615 (* 1 = 6.59615 loss)
I0223 11:57:59.313035 27291 sgd_solver.cpp:105] Iteration 960, lr = 0.01
I0223 11:58:19.114663 27291 solver.cpp:219] Iteration 980 (1.01002 iter/s, 19.8015s/20 iters), loss = 6.64156
I0223 11:58:19.138278 27291 solver.cpp:238]     Train net output #0: loss = 6.64156 (* 1 = 6.64156 loss)
I0223 11:58:19.138310 27291 sgd_solver.cpp:105] Iteration 980, lr = 0.01
I0223 11:58:37.425788 27291 solver.cpp:331] Iteration 1000, Testing net (#0)
I0223 11:58:45.099098 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0054
I0223 11:58:45.099174 27291 solver.cpp:398]     Test net output #1: loss = 6.73215 (* 1 = 6.73215 loss)
I0223 11:58:46.063655 27291 solver.cpp:219] Iteration 1000 (0.742799 iter/s, 26.9252s/20 iters), loss = 6.62666
I0223 11:58:46.063746 27291 solver.cpp:238]     Train net output #0: loss = 6.62666 (* 1 = 6.62666 loss)
I0223 11:58:46.063760 27291 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I0223 11:59:05.966943 27291 solver.cpp:219] Iteration 1020 (1.00487 iter/s, 19.903s/20 iters), loss = 6.54826
I0223 11:59:05.990556 27291 solver.cpp:238]     Train net output #0: loss = 6.54826 (* 1 = 6.54826 loss)
I0223 11:59:05.990588 27291 sgd_solver.cpp:105] Iteration 1020, lr = 0.01
I0223 11:59:25.683032 27291 solver.cpp:219] Iteration 1040 (1.01562 iter/s, 19.6923s/20 iters), loss = 6.52497
I0223 11:59:25.706656 27291 solver.cpp:238]     Train net output #0: loss = 6.52497 (* 1 = 6.52497 loss)
I0223 11:59:25.706686 27291 sgd_solver.cpp:105] Iteration 1040, lr = 0.01
I0223 11:59:45.408787 27291 solver.cpp:219] Iteration 1060 (1.01513 iter/s, 19.702s/20 iters), loss = 6.52446
I0223 11:59:45.432407 27291 solver.cpp:238]     Train net output #0: loss = 6.52446 (* 1 = 6.52446 loss)
I0223 11:59:45.432438 27291 sgd_solver.cpp:105] Iteration 1060, lr = 0.01
I0223 12:00:05.170016 27291 solver.cpp:219] Iteration 1080 (1.0133 iter/s, 19.7374s/20 iters), loss = 6.58894
I0223 12:00:05.193629 27291 solver.cpp:238]     Train net output #0: loss = 6.58894 (* 1 = 6.58894 loss)
I0223 12:00:05.193660 27291 sgd_solver.cpp:105] Iteration 1080, lr = 0.01
I0223 12:00:23.251777 27291 solver.cpp:331] Iteration 1100, Testing net (#0)
I0223 12:00:48.668632 27291 solver.cpp:398]     Test net output #0: accuracy = 0.005
I0223 12:00:48.668880 27291 solver.cpp:398]     Test net output #1: loss = 6.73256 (* 1 = 6.73256 loss)
I0223 12:00:49.640615 27291 solver.cpp:219] Iteration 1100 (0.449978 iter/s, 44.4466s/20 iters), loss = 6.47571
I0223 12:00:49.640698 27291 solver.cpp:238]     Train net output #0: loss = 6.47571 (* 1 = 6.47571 loss)
I0223 12:00:49.640712 27291 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I0223 12:01:09.335871 27291 solver.cpp:219] Iteration 1120 (1.01549 iter/s, 19.695s/20 iters), loss = 6.52117
I0223 12:01:09.359486 27291 solver.cpp:238]     Train net output #0: loss = 6.52117 (* 1 = 6.52117 loss)
I0223 12:01:09.359519 27291 sgd_solver.cpp:105] Iteration 1120, lr = 0.01
I0223 12:01:29.058935 27291 solver.cpp:219] Iteration 1140 (1.01527 iter/s, 19.6993s/20 iters), loss = 6.38193
I0223 12:01:29.082548 27291 solver.cpp:238]     Train net output #0: loss = 6.38193 (* 1 = 6.38193 loss)
I0223 12:01:29.082577 27291 sgd_solver.cpp:105] Iteration 1140, lr = 0.01
I0223 12:01:48.796093 27291 solver.cpp:219] Iteration 1160 (1.01454 iter/s, 19.7134s/20 iters), loss = 6.52458
I0223 12:01:48.819703 27291 solver.cpp:238]     Train net output #0: loss = 6.52458 (* 1 = 6.52458 loss)
I0223 12:01:48.819737 27291 sgd_solver.cpp:105] Iteration 1160, lr = 0.01
I0223 12:02:08.520994 27291 solver.cpp:219] Iteration 1180 (1.01517 iter/s, 19.7011s/20 iters), loss = 6.53222
I0223 12:02:08.544601 27291 solver.cpp:238]     Train net output #0: loss = 6.53222 (* 1 = 6.53222 loss)
I0223 12:02:08.544631 27291 sgd_solver.cpp:105] Iteration 1180, lr = 0.01
I0223 12:02:26.585145 27291 solver.cpp:331] Iteration 1200, Testing net (#0)
I0223 12:02:43.238777 27291 blocking_queue.cpp:49] Waiting for data
I0223 12:02:43.404484 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0078
I0223 12:02:43.404570 27291 solver.cpp:398]     Test net output #1: loss = 6.63842 (* 1 = 6.63842 loss)
I0223 12:02:44.370743 27291 solver.cpp:219] Iteration 1200 (0.558257 iter/s, 35.8258s/20 iters), loss = 6.52725
I0223 12:02:44.370826 27291 solver.cpp:238]     Train net output #0: loss = 6.52725 (* 1 = 6.52725 loss)
I0223 12:02:44.371045 27291 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I0223 12:03:04.198457 27291 solver.cpp:219] Iteration 1220 (1.0087 iter/s, 19.8274s/20 iters), loss = 6.45499
I0223 12:03:04.222069 27291 solver.cpp:238]     Train net output #0: loss = 6.45499 (* 1 = 6.45499 loss)
I0223 12:03:04.222100 27291 sgd_solver.cpp:105] Iteration 1220, lr = 0.01
I0223 12:03:23.925395 27291 solver.cpp:219] Iteration 1240 (1.01507 iter/s, 19.7031s/20 iters), loss = 6.54683
I0223 12:03:23.949012 27291 solver.cpp:238]     Train net output #0: loss = 6.54683 (* 1 = 6.54683 loss)
I0223 12:03:23.949041 27291 sgd_solver.cpp:105] Iteration 1240, lr = 0.01
I0223 12:03:43.650810 27291 solver.cpp:219] Iteration 1260 (1.01515 iter/s, 19.7016s/20 iters), loss = 6.34254
I0223 12:03:43.674437 27291 solver.cpp:238]     Train net output #0: loss = 6.34254 (* 1 = 6.34254 loss)
I0223 12:03:43.674468 27291 sgd_solver.cpp:105] Iteration 1260, lr = 0.01
I0223 12:04:03.380053 27291 solver.cpp:219] Iteration 1280 (1.01495 iter/s, 19.7054s/20 iters), loss = 6.53369
I0223 12:04:03.403664 27291 solver.cpp:238]     Train net output #0: loss = 6.53369 (* 1 = 6.53369 loss)
I0223 12:04:03.403695 27291 sgd_solver.cpp:105] Iteration 1280, lr = 0.01
I0223 12:04:21.452075 27291 solver.cpp:331] Iteration 1300, Testing net (#0)
I0223 12:04:42.935633 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0078
I0223 12:04:42.966859 27291 solver.cpp:398]     Test net output #1: loss = 6.64394 (* 1 = 6.64394 loss)
I0223 12:04:43.929602 27291 solver.cpp:219] Iteration 1300 (0.493517 iter/s, 40.5255s/20 iters), loss = 6.4258
I0223 12:04:43.934315 27291 solver.cpp:238]     Train net output #0: loss = 6.4258 (* 1 = 6.4258 loss)
I0223 12:04:43.934355 27291 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I0223 12:05:03.977979 27291 solver.cpp:219] Iteration 1320 (0.997833 iter/s, 20.0434s/20 iters), loss = 6.45544
I0223 12:05:03.978071 27291 solver.cpp:238]     Train net output #0: loss = 6.45544 (* 1 = 6.45544 loss)
I0223 12:05:03.978085 27291 sgd_solver.cpp:105] Iteration 1320, lr = 0.01
I0223 12:05:23.686643 27291 solver.cpp:219] Iteration 1340 (1.0148 iter/s, 19.7083s/20 iters), loss = 6.39031
I0223 12:05:23.710258 27291 solver.cpp:238]     Train net output #0: loss = 6.39031 (* 1 = 6.39031 loss)
I0223 12:05:23.710288 27291 sgd_solver.cpp:105] Iteration 1340, lr = 0.01
I0223 12:05:43.423575 27291 solver.cpp:219] Iteration 1360 (1.01455 iter/s, 19.7131s/20 iters), loss = 6.38552
I0223 12:05:43.447194 27291 solver.cpp:238]     Train net output #0: loss = 6.38552 (* 1 = 6.38552 loss)
I0223 12:05:43.447227 27291 sgd_solver.cpp:105] Iteration 1360, lr = 0.01
I0223 12:06:03.146004 27291 solver.cpp:219] Iteration 1380 (1.0153 iter/s, 19.6986s/20 iters), loss = 6.28119
I0223 12:06:03.169626 27291 solver.cpp:238]     Train net output #0: loss = 6.28119 (* 1 = 6.28119 loss)
I0223 12:06:03.169656 27291 sgd_solver.cpp:105] Iteration 1380, lr = 0.01
I0223 12:06:21.206903 27291 solver.cpp:331] Iteration 1400, Testing net (#0)
I0223 12:06:45.073895 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0084
I0223 12:06:45.074916 27291 solver.cpp:398]     Test net output #1: loss = 6.59672 (* 1 = 6.59672 loss)
I0223 12:06:46.042408 27291 solver.cpp:219] Iteration 1400 (0.466502 iter/s, 42.8723s/20 iters), loss = 6.39081
I0223 12:06:46.042500 27291 solver.cpp:238]     Train net output #0: loss = 6.39081 (* 1 = 6.39081 loss)
I0223 12:06:46.042512 27291 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I0223 12:07:05.984434 27291 solver.cpp:219] Iteration 1420 (1.00292 iter/s, 19.9417s/20 iters), loss = 6.34788
I0223 12:07:06.008051 27291 solver.cpp:238]     Train net output #0: loss = 6.34788 (* 1 = 6.34788 loss)
I0223 12:07:06.008082 27291 sgd_solver.cpp:105] Iteration 1420, lr = 0.01
I0223 12:07:25.720192 27291 solver.cpp:219] Iteration 1440 (1.01462 iter/s, 19.7119s/20 iters), loss = 6.29408
I0223 12:07:25.743808 27291 solver.cpp:238]     Train net output #0: loss = 6.29408 (* 1 = 6.29408 loss)
I0223 12:07:25.743836 27291 sgd_solver.cpp:105] Iteration 1440, lr = 0.01
I0223 12:07:45.443276 27291 solver.cpp:219] Iteration 1460 (1.01527 iter/s, 19.6992s/20 iters), loss = 6.28824
I0223 12:07:45.466897 27291 solver.cpp:238]     Train net output #0: loss = 6.28824 (* 1 = 6.28824 loss)
I0223 12:07:45.466928 27291 sgd_solver.cpp:105] Iteration 1460, lr = 0.01
I0223 12:08:05.152143 27291 solver.cpp:219] Iteration 1480 (1.016 iter/s, 19.685s/20 iters), loss = 6.2474
I0223 12:08:05.175762 27291 solver.cpp:238]     Train net output #0: loss = 6.2474 (* 1 = 6.2474 loss)
I0223 12:08:05.175793 27291 sgd_solver.cpp:105] Iteration 1480, lr = 0.01
I0223 12:08:23.209216 27291 solver.cpp:331] Iteration 1500, Testing net (#0)
I0223 12:08:41.263638 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0118
I0223 12:08:41.264706 27291 solver.cpp:398]     Test net output #1: loss = 6.47167 (* 1 = 6.47167 loss)
I0223 12:08:42.232714 27291 solver.cpp:219] Iteration 1500 (0.539717 iter/s, 37.0565s/20 iters), loss = 6.36972
I0223 12:08:42.232802 27291 solver.cpp:238]     Train net output #0: loss = 6.36972 (* 1 = 6.36972 loss)
I0223 12:08:42.232816 27291 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I0223 12:09:02.278744 27291 solver.cpp:219] Iteration 1520 (0.997721 iter/s, 20.0457s/20 iters), loss = 6.32646
I0223 12:09:02.302362 27291 solver.cpp:238]     Train net output #0: loss = 6.32646 (* 1 = 6.32646 loss)
I0223 12:09:02.302393 27291 sgd_solver.cpp:105] Iteration 1520, lr = 0.01
I0223 12:09:22.186765 27291 solver.cpp:219] Iteration 1540 (1.00583 iter/s, 19.8841s/20 iters), loss = 6.29544
I0223 12:09:22.210381 27291 solver.cpp:238]     Train net output #0: loss = 6.29544 (* 1 = 6.29544 loss)
I0223 12:09:22.210423 27291 sgd_solver.cpp:105] Iteration 1540, lr = 0.01
I0223 12:09:41.915839 27291 solver.cpp:219] Iteration 1560 (1.01496 iter/s, 19.7052s/20 iters), loss = 6.24192
I0223 12:09:41.939451 27291 solver.cpp:238]     Train net output #0: loss = 6.24192 (* 1 = 6.24192 loss)
I0223 12:09:41.939483 27291 sgd_solver.cpp:105] Iteration 1560, lr = 0.01
I0223 12:10:01.638238 27291 solver.cpp:219] Iteration 1580 (1.0153 iter/s, 19.6985s/20 iters), loss = 6.31651
I0223 12:10:01.661859 27291 solver.cpp:238]     Train net output #0: loss = 6.31651 (* 1 = 6.31651 loss)
I0223 12:10:01.661890 27291 sgd_solver.cpp:105] Iteration 1580, lr = 0.01
I0223 12:10:19.700976 27291 solver.cpp:331] Iteration 1600, Testing net (#0)
I0223 12:10:35.230624 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0124
I0223 12:10:35.230799 27291 solver.cpp:398]     Test net output #1: loss = 6.35296 (* 1 = 6.35296 loss)
I0223 12:10:36.199980 27291 solver.cpp:219] Iteration 1600 (0.579078 iter/s, 34.5377s/20 iters), loss = 6.17752
I0223 12:10:36.200052 27291 solver.cpp:238]     Train net output #0: loss = 6.17752 (* 1 = 6.17752 loss)
I0223 12:10:36.200067 27291 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I0223 12:10:55.955384 27291 solver.cpp:219] Iteration 1620 (1.0124 iter/s, 19.7551s/20 iters), loss = 6.19068
I0223 12:10:55.979003 27291 solver.cpp:238]     Train net output #0: loss = 6.19068 (* 1 = 6.19068 loss)
I0223 12:10:55.979034 27291 sgd_solver.cpp:105] Iteration 1620, lr = 0.01
I0223 12:11:15.677402 27291 solver.cpp:219] Iteration 1640 (1.01532 iter/s, 19.6981s/20 iters), loss = 6.11688
I0223 12:11:15.701020 27291 solver.cpp:238]     Train net output #0: loss = 6.11688 (* 1 = 6.11688 loss)
I0223 12:11:15.701050 27291 sgd_solver.cpp:105] Iteration 1640, lr = 0.01
I0223 12:11:35.381263 27291 solver.cpp:219] Iteration 1660 (1.01626 iter/s, 19.68s/20 iters), loss = 6.21372
I0223 12:11:35.404880 27291 solver.cpp:238]     Train net output #0: loss = 6.21372 (* 1 = 6.21372 loss)
I0223 12:11:35.404911 27291 sgd_solver.cpp:105] Iteration 1660, lr = 0.01
I0223 12:11:55.094094 27291 solver.cpp:219] Iteration 1680 (1.0158 iter/s, 19.6889s/20 iters), loss = 6.1656
I0223 12:11:55.117708 27291 solver.cpp:238]     Train net output #0: loss = 6.1656 (* 1 = 6.1656 loss)
I0223 12:11:55.117738 27291 sgd_solver.cpp:105] Iteration 1680, lr = 0.01
I0223 12:12:13.182711 27291 solver.cpp:331] Iteration 1700, Testing net (#0)
I0223 12:12:29.066674 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0136
I0223 12:12:29.066854 27291 solver.cpp:398]     Test net output #1: loss = 6.30453 (* 1 = 6.30453 loss)
I0223 12:12:30.035598 27291 solver.cpp:219] Iteration 1700 (0.57278 iter/s, 34.9174s/20 iters), loss = 6.01456
I0223 12:12:30.040246 27291 solver.cpp:238]     Train net output #0: loss = 6.01456 (* 1 = 6.01456 loss)
I0223 12:12:30.040277 27291 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I0223 12:12:49.843044 27291 solver.cpp:219] Iteration 1720 (1.00997 iter/s, 19.8025s/20 iters), loss = 6.04894
I0223 12:12:49.866662 27291 solver.cpp:238]     Train net output #0: loss = 6.04894 (* 1 = 6.04894 loss)
I0223 12:12:49.866694 27291 sgd_solver.cpp:105] Iteration 1720, lr = 0.01
I0223 12:13:09.558451 27291 solver.cpp:219] Iteration 1740 (1.01567 iter/s, 19.6915s/20 iters), loss = 6.12034
I0223 12:13:09.582073 27291 solver.cpp:238]     Train net output #0: loss = 6.12034 (* 1 = 6.12034 loss)
I0223 12:13:09.582104 27291 sgd_solver.cpp:105] Iteration 1740, lr = 0.01
I0223 12:13:29.277256 27291 solver.cpp:219] Iteration 1760 (1.01549 iter/s, 19.6949s/20 iters), loss = 5.97291
I0223 12:13:29.300868 27291 solver.cpp:238]     Train net output #0: loss = 5.97291 (* 1 = 5.97291 loss)
I0223 12:13:29.300899 27291 sgd_solver.cpp:105] Iteration 1760, lr = 0.01
I0223 12:13:49.006522 27291 solver.cpp:219] Iteration 1780 (1.01495 iter/s, 19.7054s/20 iters), loss = 6.05002
I0223 12:13:49.030135 27291 solver.cpp:238]     Train net output #0: loss = 6.05002 (* 1 = 6.05002 loss)
I0223 12:13:49.030166 27291 sgd_solver.cpp:105] Iteration 1780, lr = 0.01
I0223 12:14:07.070996 27291 solver.cpp:331] Iteration 1800, Testing net (#0)
I0223 12:14:20.239213 27291 blocking_queue.cpp:49] Waiting for data
I0223 12:14:25.400879 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0204
I0223 12:14:25.400997 27291 solver.cpp:398]     Test net output #1: loss = 6.22629 (* 1 = 6.22629 loss)
I0223 12:14:26.363415 27291 solver.cpp:219] Iteration 1800 (0.535723 iter/s, 37.3327s/20 iters), loss = 6.06608
I0223 12:14:26.368077 27291 solver.cpp:238]     Train net output #0: loss = 6.06608 (* 1 = 6.06608 loss)
I0223 12:14:26.368116 27291 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I0223 12:14:46.422276 27291 solver.cpp:219] Iteration 1820 (0.997311 iter/s, 20.0539s/20 iters), loss = 6.02394
I0223 12:14:46.445865 27291 solver.cpp:238]     Train net output #0: loss = 6.02394 (* 1 = 6.02394 loss)
I0223 12:14:46.445888 27291 sgd_solver.cpp:105] Iteration 1820, lr = 0.01
I0223 12:15:06.488323 27291 solver.cpp:219] Iteration 1840 (0.997896 iter/s, 20.0422s/20 iters), loss = 6.00499
I0223 12:15:06.488577 27291 solver.cpp:238]     Train net output #0: loss = 6.00499 (* 1 = 6.00499 loss)
I0223 12:15:06.488595 27291 sgd_solver.cpp:105] Iteration 1840, lr = 0.01
I0223 12:15:26.270534 27291 solver.cpp:219] Iteration 1860 (1.01107 iter/s, 19.7811s/20 iters), loss = 6.00781
I0223 12:15:26.294148 27291 solver.cpp:238]     Train net output #0: loss = 6.00781 (* 1 = 6.00781 loss)
I0223 12:15:26.294181 27291 sgd_solver.cpp:105] Iteration 1860, lr = 0.01
I0223 12:15:45.994665 27291 solver.cpp:219] Iteration 1880 (1.01522 iter/s, 19.7002s/20 iters), loss = 5.87016
I0223 12:15:46.018282 27291 solver.cpp:238]     Train net output #0: loss = 5.87016 (* 1 = 5.87016 loss)
I0223 12:15:46.018311 27291 sgd_solver.cpp:105] Iteration 1880, lr = 0.01
I0223 12:16:04.061319 27291 solver.cpp:331] Iteration 1900, Testing net (#0)
I0223 12:16:24.146136 27318 data_layer.cpp:73] Restarting data prefetching from start.
I0223 12:16:24.226274 27291 solver.cpp:398]     Test net output #0: accuracy = 0.02
I0223 12:16:24.226337 27291 solver.cpp:398]     Test net output #1: loss = 6.16527 (* 1 = 6.16527 loss)
I0223 12:16:25.194993 27291 solver.cpp:219] Iteration 1900 (0.510515 iter/s, 39.1762s/20 iters), loss = 6.07724
I0223 12:16:25.195077 27291 solver.cpp:238]     Train net output #0: loss = 6.07724 (* 1 = 6.07724 loss)
I0223 12:16:25.195091 27291 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I0223 12:16:45.201872 27291 solver.cpp:219] Iteration 1920 (0.999675 iter/s, 20.0065s/20 iters), loss = 5.8779
I0223 12:16:45.225491 27291 solver.cpp:238]     Train net output #0: loss = 5.8779 (* 1 = 5.8779 loss)
I0223 12:16:45.225520 27291 sgd_solver.cpp:105] Iteration 1920, lr = 0.01
I0223 12:17:05.121937 27291 solver.cpp:219] Iteration 1940 (1.00522 iter/s, 19.8962s/20 iters), loss = 5.98136
I0223 12:17:05.145553 27291 solver.cpp:238]     Train net output #0: loss = 5.98136 (* 1 = 5.98136 loss)
I0223 12:17:05.145583 27291 sgd_solver.cpp:105] Iteration 1940, lr = 0.01
I0223 12:17:24.830651 27291 solver.cpp:219] Iteration 1960 (1.01601 iter/s, 19.6848s/20 iters), loss = 6.02792
I0223 12:17:24.854271 27291 solver.cpp:238]     Train net output #0: loss = 6.02792 (* 1 = 6.02792 loss)
I0223 12:17:24.854305 27291 sgd_solver.cpp:105] Iteration 1960, lr = 0.01
I0223 12:17:44.540622 27291 solver.cpp:219] Iteration 1980 (1.01595 iter/s, 19.6861s/20 iters), loss = 5.97885
I0223 12:17:44.564246 27291 solver.cpp:238]     Train net output #0: loss = 5.97885 (* 1 = 5.97885 loss)
I0223 12:17:44.564277 27291 sgd_solver.cpp:105] Iteration 1980, lr = 0.01
I0223 12:18:02.602983 27291 solver.cpp:448] Snapshotting to binary proto file models/caffenet_proj/caffenet_train_iter_2000.caffemodel
I0223 12:18:08.566278 27291 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/caffenet_proj/caffenet_train_iter_2000.solverstate
I0223 12:18:11.968391 27291 solver.cpp:311] Iteration 2000, loss = 5.86741
I0223 12:18:11.968441 27291 solver.cpp:331] Iteration 2000, Testing net (#0)
I0223 12:18:18.937171 27291 solver.cpp:398]     Test net output #0: accuracy = 0.0282
I0223 12:18:18.937402 27291 solver.cpp:398]     Test net output #1: loss = 6.03714 (* 1 = 6.03714 loss)
I0223 12:18:18.937414 27291 solver.cpp:316] Optimization Done.
I0223 12:18:18.937422 27291 caffe.cpp:259] Optimization Done.
