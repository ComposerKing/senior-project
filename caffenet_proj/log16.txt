I0329 22:19:25.380084 16096 caffe.cpp:218] Using GPUs 0
I0329 22:19:25.423238 16096 caffe.cpp:223] GPU 0: Tesla K20c
I0329 22:19:25.760663 16096 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 100
base_lr: 0.1
display: 20
max_iter: 3000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "models/caffenet_proj/caffenet_train"
solver_mode: GPU
device_id: 0
net: "models/caffenet_proj/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I0329 22:19:25.760848 16096 solver.cpp:87] Creating training net from net file: models/caffenet_proj/train_val.prototxt
I0329 22:19:25.787087 16096 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0329 22:19:25.787144 16096 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0329 22:19:25.787544 16096 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "examples/imagenet/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0329 22:19:25.787750 16096 layer_factory.hpp:77] Creating layer data
I0329 22:19:25.787956 16096 db_lmdb.cpp:35] Opened lmdb examples/imagenet/ilsvrc12_train_lmdb
I0329 22:19:25.815311 16096 net.cpp:86] Creating Layer data
I0329 22:19:25.815366 16096 net.cpp:382] data -> data
I0329 22:19:25.815428 16096 net.cpp:382] data -> label
I0329 22:19:25.815465 16096 data_transformer.cpp:25] Loading mean file from: data/ilsvrc12/imagenet_mean.binaryproto
I0329 22:19:25.876457 16096 data_layer.cpp:45] output data size: 256,3,227,227
I0329 22:19:26.467491 16096 net.cpp:124] Setting up data
I0329 22:19:26.467548 16096 net.cpp:131] Top shape: 256 3 227 227 (39574272)
I0329 22:19:26.467557 16096 net.cpp:131] Top shape: 256 (256)
I0329 22:19:26.467562 16096 net.cpp:139] Memory required for data: 158298112
I0329 22:19:26.467577 16096 layer_factory.hpp:77] Creating layer conv1
I0329 22:19:26.467609 16096 net.cpp:86] Creating Layer conv1
I0329 22:19:26.467619 16096 net.cpp:408] conv1 <- data
I0329 22:19:26.467639 16096 net.cpp:382] conv1 -> conv1
I0329 22:19:27.022469 16096 net.cpp:124] Setting up conv1
I0329 22:19:27.022518 16096 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0329 22:19:27.022526 16096 net.cpp:139] Memory required for data: 455667712
I0329 22:19:27.022573 16096 layer_factory.hpp:77] Creating layer relu1
I0329 22:19:27.022593 16096 net.cpp:86] Creating Layer relu1
I0329 22:19:27.022601 16096 net.cpp:408] relu1 <- conv1
I0329 22:19:27.022611 16096 net.cpp:369] relu1 -> conv1 (in-place)
I0329 22:19:27.023020 16096 net.cpp:124] Setting up relu1
I0329 22:19:27.023036 16096 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0329 22:19:27.023042 16096 net.cpp:139] Memory required for data: 753037312
I0329 22:19:27.023048 16096 layer_factory.hpp:77] Creating layer pool1
I0329 22:19:27.023059 16096 net.cpp:86] Creating Layer pool1
I0329 22:19:27.023066 16096 net.cpp:408] pool1 <- conv1
I0329 22:19:27.023075 16096 net.cpp:382] pool1 -> pool1
I0329 22:19:27.023136 16096 net.cpp:124] Setting up pool1
I0329 22:19:27.023149 16096 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0329 22:19:27.023154 16096 net.cpp:139] Memory required for data: 824700928
I0329 22:19:27.023159 16096 layer_factory.hpp:77] Creating layer norm1
I0329 22:19:27.023175 16096 net.cpp:86] Creating Layer norm1
I0329 22:19:27.023191 16096 net.cpp:408] norm1 <- pool1
I0329 22:19:27.023212 16096 net.cpp:382] norm1 -> norm1
I0329 22:19:27.023450 16096 net.cpp:124] Setting up norm1
I0329 22:19:27.023465 16096 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0329 22:19:27.023470 16096 net.cpp:139] Memory required for data: 896364544
I0329 22:19:27.023476 16096 layer_factory.hpp:77] Creating layer conv2
I0329 22:19:27.023494 16096 net.cpp:86] Creating Layer conv2
I0329 22:19:27.023500 16096 net.cpp:408] conv2 <- norm1
I0329 22:19:27.023510 16096 net.cpp:382] conv2 -> conv2
I0329 22:19:27.031533 16096 net.cpp:124] Setting up conv2
I0329 22:19:27.031569 16096 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0329 22:19:27.031575 16096 net.cpp:139] Memory required for data: 1087467520
I0329 22:19:27.031594 16096 layer_factory.hpp:77] Creating layer relu2
I0329 22:19:27.031605 16096 net.cpp:86] Creating Layer relu2
I0329 22:19:27.031616 16096 net.cpp:408] relu2 <- conv2
I0329 22:19:27.031626 16096 net.cpp:369] relu2 -> conv2 (in-place)
I0329 22:19:27.031850 16096 net.cpp:124] Setting up relu2
I0329 22:19:27.031867 16096 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0329 22:19:27.031872 16096 net.cpp:139] Memory required for data: 1278570496
I0329 22:19:27.031878 16096 layer_factory.hpp:77] Creating layer pool2
I0329 22:19:27.031889 16096 net.cpp:86] Creating Layer pool2
I0329 22:19:27.031895 16096 net.cpp:408] pool2 <- conv2
I0329 22:19:27.031904 16096 net.cpp:382] pool2 -> pool2
I0329 22:19:27.031961 16096 net.cpp:124] Setting up pool2
I0329 22:19:27.031972 16096 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0329 22:19:27.031978 16096 net.cpp:139] Memory required for data: 1322872832
I0329 22:19:27.031983 16096 layer_factory.hpp:77] Creating layer norm2
I0329 22:19:27.031998 16096 net.cpp:86] Creating Layer norm2
I0329 22:19:27.032004 16096 net.cpp:408] norm2 <- pool2
I0329 22:19:27.032013 16096 net.cpp:382] norm2 -> norm2
I0329 22:19:27.032515 16096 net.cpp:124] Setting up norm2
I0329 22:19:27.032533 16096 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0329 22:19:27.032539 16096 net.cpp:139] Memory required for data: 1367175168
I0329 22:19:27.032562 16096 layer_factory.hpp:77] Creating layer conv3
I0329 22:19:27.032595 16096 net.cpp:86] Creating Layer conv3
I0329 22:19:27.032609 16096 net.cpp:408] conv3 <- norm2
I0329 22:19:27.032629 16096 net.cpp:382] conv3 -> conv3
I0329 22:19:27.053499 16096 net.cpp:124] Setting up conv3
I0329 22:19:27.053547 16096 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0329 22:19:27.053553 16096 net.cpp:139] Memory required for data: 1433628672
I0329 22:19:27.053587 16096 layer_factory.hpp:77] Creating layer relu3
I0329 22:19:27.053604 16096 net.cpp:86] Creating Layer relu3
I0329 22:19:27.053613 16096 net.cpp:408] relu3 <- conv3
I0329 22:19:27.053625 16096 net.cpp:369] relu3 -> conv3 (in-place)
I0329 22:19:27.053889 16096 net.cpp:124] Setting up relu3
I0329 22:19:27.053908 16096 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0329 22:19:27.053916 16096 net.cpp:139] Memory required for data: 1500082176
I0329 22:19:27.053923 16096 layer_factory.hpp:77] Creating layer conv4
I0329 22:19:27.053943 16096 net.cpp:86] Creating Layer conv4
I0329 22:19:27.053951 16096 net.cpp:408] conv4 <- conv3
I0329 22:19:27.053962 16096 net.cpp:382] conv4 -> conv4
I0329 22:19:27.068037 16096 net.cpp:124] Setting up conv4
I0329 22:19:27.068081 16096 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0329 22:19:27.068089 16096 net.cpp:139] Memory required for data: 1566535680
I0329 22:19:27.068102 16096 layer_factory.hpp:77] Creating layer relu4
I0329 22:19:27.068116 16096 net.cpp:86] Creating Layer relu4
I0329 22:19:27.068123 16096 net.cpp:408] relu4 <- conv4
I0329 22:19:27.068137 16096 net.cpp:369] relu4 -> conv4 (in-place)
I0329 22:19:27.068369 16096 net.cpp:124] Setting up relu4
I0329 22:19:27.068385 16096 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0329 22:19:27.068390 16096 net.cpp:139] Memory required for data: 1632989184
I0329 22:19:27.068397 16096 layer_factory.hpp:77] Creating layer conv5
I0329 22:19:27.068425 16096 net.cpp:86] Creating Layer conv5
I0329 22:19:27.068454 16096 net.cpp:408] conv5 <- conv4
I0329 22:19:27.068465 16096 net.cpp:382] conv5 -> conv5
I0329 22:19:27.078694 16096 net.cpp:124] Setting up conv5
I0329 22:19:27.078727 16096 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0329 22:19:27.078734 16096 net.cpp:139] Memory required for data: 1677291520
I0329 22:19:27.078752 16096 layer_factory.hpp:77] Creating layer relu5
I0329 22:19:27.078766 16096 net.cpp:86] Creating Layer relu5
I0329 22:19:27.078773 16096 net.cpp:408] relu5 <- conv5
I0329 22:19:27.078784 16096 net.cpp:369] relu5 -> conv5 (in-place)
I0329 22:19:27.079007 16096 net.cpp:124] Setting up relu5
I0329 22:19:27.079021 16096 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0329 22:19:27.079027 16096 net.cpp:139] Memory required for data: 1721593856
I0329 22:19:27.079033 16096 layer_factory.hpp:77] Creating layer pool5
I0329 22:19:27.079046 16096 net.cpp:86] Creating Layer pool5
I0329 22:19:27.079053 16096 net.cpp:408] pool5 <- conv5
I0329 22:19:27.079064 16096 net.cpp:382] pool5 -> pool5
I0329 22:19:27.079123 16096 net.cpp:124] Setting up pool5
I0329 22:19:27.079134 16096 net.cpp:131] Top shape: 256 256 6 6 (2359296)
I0329 22:19:27.079140 16096 net.cpp:139] Memory required for data: 1731031040
I0329 22:19:27.079146 16096 layer_factory.hpp:77] Creating layer fc6
I0329 22:19:27.079164 16096 net.cpp:86] Creating Layer fc6
I0329 22:19:27.079170 16096 net.cpp:408] fc6 <- pool5
I0329 22:19:27.079180 16096 net.cpp:382] fc6 -> fc6
I0329 22:19:27.671298 16096 net.cpp:124] Setting up fc6
I0329 22:19:27.671344 16096 net.cpp:131] Top shape: 256 4096 (1048576)
I0329 22:19:27.671350 16096 net.cpp:139] Memory required for data: 1735225344
I0329 22:19:27.671365 16096 layer_factory.hpp:77] Creating layer relu6
I0329 22:19:27.671378 16096 net.cpp:86] Creating Layer relu6
I0329 22:19:27.671386 16096 net.cpp:408] relu6 <- fc6
I0329 22:19:27.671397 16096 net.cpp:369] relu6 -> fc6 (in-place)
I0329 22:19:27.671968 16096 net.cpp:124] Setting up relu6
I0329 22:19:27.671984 16096 net.cpp:131] Top shape: 256 4096 (1048576)
I0329 22:19:27.671990 16096 net.cpp:139] Memory required for data: 1739419648
I0329 22:19:27.671996 16096 layer_factory.hpp:77] Creating layer drop6
I0329 22:19:27.672018 16096 net.cpp:86] Creating Layer drop6
I0329 22:19:27.672024 16096 net.cpp:408] drop6 <- fc6
I0329 22:19:27.672032 16096 net.cpp:369] drop6 -> fc6 (in-place)
I0329 22:19:27.672072 16096 net.cpp:124] Setting up drop6
I0329 22:19:27.672083 16096 net.cpp:131] Top shape: 256 4096 (1048576)
I0329 22:19:27.672088 16096 net.cpp:139] Memory required for data: 1743613952
I0329 22:19:27.672094 16096 layer_factory.hpp:77] Creating layer fc7
I0329 22:19:27.672106 16096 net.cpp:86] Creating Layer fc7
I0329 22:19:27.672111 16096 net.cpp:408] fc7 <- fc6
I0329 22:19:27.672122 16096 net.cpp:382] fc7 -> fc7
I0329 22:19:27.935652 16096 net.cpp:124] Setting up fc7
I0329 22:19:27.935698 16096 net.cpp:131] Top shape: 256 4096 (1048576)
I0329 22:19:27.935704 16096 net.cpp:139] Memory required for data: 1747808256
I0329 22:19:27.935719 16096 layer_factory.hpp:77] Creating layer relu7
I0329 22:19:27.935734 16096 net.cpp:86] Creating Layer relu7
I0329 22:19:27.935740 16096 net.cpp:408] relu7 <- fc7
I0329 22:19:27.935753 16096 net.cpp:369] relu7 -> fc7 (in-place)
I0329 22:19:27.936055 16096 net.cpp:124] Setting up relu7
I0329 22:19:27.936070 16096 net.cpp:131] Top shape: 256 4096 (1048576)
I0329 22:19:27.936075 16096 net.cpp:139] Memory required for data: 1752002560
I0329 22:19:27.936082 16096 layer_factory.hpp:77] Creating layer drop7
I0329 22:19:27.936092 16096 net.cpp:86] Creating Layer drop7
I0329 22:19:27.936100 16096 net.cpp:408] drop7 <- fc7
I0329 22:19:27.936110 16096 net.cpp:369] drop7 -> fc7 (in-place)
I0329 22:19:27.936141 16096 net.cpp:124] Setting up drop7
I0329 22:19:27.936151 16096 net.cpp:131] Top shape: 256 4096 (1048576)
I0329 22:19:27.936157 16096 net.cpp:139] Memory required for data: 1756196864
I0329 22:19:27.936162 16096 layer_factory.hpp:77] Creating layer fc8
I0329 22:19:27.936178 16096 net.cpp:86] Creating Layer fc8
I0329 22:19:27.936192 16096 net.cpp:408] fc8 <- fc7
I0329 22:19:27.936216 16096 net.cpp:382] fc8 -> fc8
I0329 22:19:28.000879 16096 net.cpp:124] Setting up fc8
I0329 22:19:28.000924 16096 net.cpp:131] Top shape: 256 1000 (256000)
I0329 22:19:28.000931 16096 net.cpp:139] Memory required for data: 1757220864
I0329 22:19:28.000946 16096 layer_factory.hpp:77] Creating layer loss
I0329 22:19:28.000960 16096 net.cpp:86] Creating Layer loss
I0329 22:19:28.000967 16096 net.cpp:408] loss <- fc8
I0329 22:19:28.000977 16096 net.cpp:408] loss <- label
I0329 22:19:28.000990 16096 net.cpp:382] loss -> loss
I0329 22:19:28.001013 16096 layer_factory.hpp:77] Creating layer loss
I0329 22:19:28.002357 16096 net.cpp:124] Setting up loss
I0329 22:19:28.002374 16096 net.cpp:131] Top shape: (1)
I0329 22:19:28.002380 16096 net.cpp:134]     with loss weight 1
I0329 22:19:28.002419 16096 net.cpp:139] Memory required for data: 1757220868
I0329 22:19:28.002426 16096 net.cpp:200] loss needs backward computation.
I0329 22:19:28.002437 16096 net.cpp:200] fc8 needs backward computation.
I0329 22:19:28.002444 16096 net.cpp:200] drop7 needs backward computation.
I0329 22:19:28.002449 16096 net.cpp:200] relu7 needs backward computation.
I0329 22:19:28.002455 16096 net.cpp:200] fc7 needs backward computation.
I0329 22:19:28.002461 16096 net.cpp:200] drop6 needs backward computation.
I0329 22:19:28.002466 16096 net.cpp:200] relu6 needs backward computation.
I0329 22:19:28.002472 16096 net.cpp:200] fc6 needs backward computation.
I0329 22:19:28.002478 16096 net.cpp:200] pool5 needs backward computation.
I0329 22:19:28.002485 16096 net.cpp:200] relu5 needs backward computation.
I0329 22:19:28.002490 16096 net.cpp:200] conv5 needs backward computation.
I0329 22:19:28.002496 16096 net.cpp:200] relu4 needs backward computation.
I0329 22:19:28.002501 16096 net.cpp:200] conv4 needs backward computation.
I0329 22:19:28.002506 16096 net.cpp:200] relu3 needs backward computation.
I0329 22:19:28.002512 16096 net.cpp:200] conv3 needs backward computation.
I0329 22:19:28.002518 16096 net.cpp:200] norm2 needs backward computation.
I0329 22:19:28.002524 16096 net.cpp:200] pool2 needs backward computation.
I0329 22:19:28.002531 16096 net.cpp:200] relu2 needs backward computation.
I0329 22:19:28.002535 16096 net.cpp:200] conv2 needs backward computation.
I0329 22:19:28.002542 16096 net.cpp:200] norm1 needs backward computation.
I0329 22:19:28.002547 16096 net.cpp:200] pool1 needs backward computation.
I0329 22:19:28.002552 16096 net.cpp:200] relu1 needs backward computation.
I0329 22:19:28.002558 16096 net.cpp:200] conv1 needs backward computation.
I0329 22:19:28.002564 16096 net.cpp:202] data does not need backward computation.
I0329 22:19:28.002570 16096 net.cpp:244] This network produces output loss
I0329 22:19:28.002593 16096 net.cpp:257] Network initialization done.
I0329 22:19:28.002966 16096 solver.cpp:173] Creating test net (#0) specified by net file: models/caffenet_proj/train_val.prototxt
I0329 22:19:28.003013 16096 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0329 22:19:28.003255 16096 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "examples/imagenet/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0329 22:19:28.003414 16096 layer_factory.hpp:77] Creating layer data
I0329 22:19:28.003509 16096 db_lmdb.cpp:35] Opened lmdb examples/imagenet/ilsvrc12_val_lmdb
I0329 22:19:28.003536 16096 net.cpp:86] Creating Layer data
I0329 22:19:28.003546 16096 net.cpp:382] data -> data
I0329 22:19:28.003563 16096 net.cpp:382] data -> label
I0329 22:19:28.003576 16096 data_transformer.cpp:25] Loading mean file from: data/ilsvrc12/imagenet_mean.binaryproto
I0329 22:19:28.006073 16096 data_layer.cpp:45] output data size: 50,3,227,227
I0329 22:19:28.087416 16096 net.cpp:124] Setting up data
I0329 22:19:28.087468 16096 net.cpp:131] Top shape: 50 3 227 227 (7729350)
I0329 22:19:28.087477 16096 net.cpp:131] Top shape: 50 (50)
I0329 22:19:28.087482 16096 net.cpp:139] Memory required for data: 30917600
I0329 22:19:28.087492 16096 layer_factory.hpp:77] Creating layer label_data_1_split
I0329 22:19:28.087510 16096 net.cpp:86] Creating Layer label_data_1_split
I0329 22:19:28.087518 16096 net.cpp:408] label_data_1_split <- label
I0329 22:19:28.087529 16096 net.cpp:382] label_data_1_split -> label_data_1_split_0
I0329 22:19:28.087548 16096 net.cpp:382] label_data_1_split -> label_data_1_split_1
I0329 22:19:28.087646 16096 net.cpp:124] Setting up label_data_1_split
I0329 22:19:28.087659 16096 net.cpp:131] Top shape: 50 (50)
I0329 22:19:28.087666 16096 net.cpp:131] Top shape: 50 (50)
I0329 22:19:28.087671 16096 net.cpp:139] Memory required for data: 30918000
I0329 22:19:28.087677 16096 layer_factory.hpp:77] Creating layer conv1
I0329 22:19:28.087695 16096 net.cpp:86] Creating Layer conv1
I0329 22:19:28.087702 16096 net.cpp:408] conv1 <- data
I0329 22:19:28.087713 16096 net.cpp:382] conv1 -> conv1
I0329 22:19:28.094535 16096 net.cpp:124] Setting up conv1
I0329 22:19:28.094578 16096 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0329 22:19:28.094584 16096 net.cpp:139] Memory required for data: 88998000
I0329 22:19:28.094605 16096 layer_factory.hpp:77] Creating layer relu1
I0329 22:19:28.094617 16096 net.cpp:86] Creating Layer relu1
I0329 22:19:28.094624 16096 net.cpp:408] relu1 <- conv1
I0329 22:19:28.094635 16096 net.cpp:369] relu1 -> conv1 (in-place)
I0329 22:19:28.094852 16096 net.cpp:124] Setting up relu1
I0329 22:19:28.094867 16096 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0329 22:19:28.094873 16096 net.cpp:139] Memory required for data: 147078000
I0329 22:19:28.094879 16096 layer_factory.hpp:77] Creating layer pool1
I0329 22:19:28.094892 16096 net.cpp:86] Creating Layer pool1
I0329 22:19:28.094899 16096 net.cpp:408] pool1 <- conv1
I0329 22:19:28.094908 16096 net.cpp:382] pool1 -> pool1
I0329 22:19:28.094964 16096 net.cpp:124] Setting up pool1
I0329 22:19:28.094976 16096 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0329 22:19:28.094981 16096 net.cpp:139] Memory required for data: 161074800
I0329 22:19:28.094987 16096 layer_factory.hpp:77] Creating layer norm1
I0329 22:19:28.094998 16096 net.cpp:86] Creating Layer norm1
I0329 22:19:28.095005 16096 net.cpp:408] norm1 <- pool1
I0329 22:19:28.095013 16096 net.cpp:382] norm1 -> norm1
I0329 22:19:28.095459 16096 net.cpp:124] Setting up norm1
I0329 22:19:28.095476 16096 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0329 22:19:28.095482 16096 net.cpp:139] Memory required for data: 175071600
I0329 22:19:28.095489 16096 layer_factory.hpp:77] Creating layer conv2
I0329 22:19:28.095506 16096 net.cpp:86] Creating Layer conv2
I0329 22:19:28.095512 16096 net.cpp:408] conv2 <- norm1
I0329 22:19:28.095522 16096 net.cpp:382] conv2 -> conv2
I0329 22:19:28.103070 16096 net.cpp:124] Setting up conv2
I0329 22:19:28.103121 16096 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0329 22:19:28.103128 16096 net.cpp:139] Memory required for data: 212396400
I0329 22:19:28.103149 16096 layer_factory.hpp:77] Creating layer relu2
I0329 22:19:28.103165 16096 net.cpp:86] Creating Layer relu2
I0329 22:19:28.103173 16096 net.cpp:408] relu2 <- conv2
I0329 22:19:28.103184 16096 net.cpp:369] relu2 -> conv2 (in-place)
I0329 22:19:28.103412 16096 net.cpp:124] Setting up relu2
I0329 22:19:28.103437 16096 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0329 22:19:28.103456 16096 net.cpp:139] Memory required for data: 249721200
I0329 22:19:28.103461 16096 layer_factory.hpp:77] Creating layer pool2
I0329 22:19:28.103476 16096 net.cpp:86] Creating Layer pool2
I0329 22:19:28.103482 16096 net.cpp:408] pool2 <- conv2
I0329 22:19:28.103490 16096 net.cpp:382] pool2 -> pool2
I0329 22:19:28.103552 16096 net.cpp:124] Setting up pool2
I0329 22:19:28.103564 16096 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0329 22:19:28.103570 16096 net.cpp:139] Memory required for data: 258374000
I0329 22:19:28.103575 16096 layer_factory.hpp:77] Creating layer norm2
I0329 22:19:28.103586 16096 net.cpp:86] Creating Layer norm2
I0329 22:19:28.103592 16096 net.cpp:408] norm2 <- pool2
I0329 22:19:28.103601 16096 net.cpp:382] norm2 -> norm2
I0329 22:19:28.104061 16096 net.cpp:124] Setting up norm2
I0329 22:19:28.104079 16096 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0329 22:19:28.104084 16096 net.cpp:139] Memory required for data: 267026800
I0329 22:19:28.104090 16096 layer_factory.hpp:77] Creating layer conv3
I0329 22:19:28.104107 16096 net.cpp:86] Creating Layer conv3
I0329 22:19:28.104113 16096 net.cpp:408] conv3 <- norm2
I0329 22:19:28.104125 16096 net.cpp:382] conv3 -> conv3
I0329 22:19:28.119585 16096 net.cpp:124] Setting up conv3
I0329 22:19:28.119637 16096 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0329 22:19:28.119642 16096 net.cpp:139] Memory required for data: 280006000
I0329 22:19:28.119664 16096 layer_factory.hpp:77] Creating layer relu3
I0329 22:19:28.119679 16096 net.cpp:86] Creating Layer relu3
I0329 22:19:28.119690 16096 net.cpp:408] relu3 <- conv3
I0329 22:19:28.119701 16096 net.cpp:369] relu3 -> conv3 (in-place)
I0329 22:19:28.120127 16096 net.cpp:124] Setting up relu3
I0329 22:19:28.120144 16096 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0329 22:19:28.120149 16096 net.cpp:139] Memory required for data: 292985200
I0329 22:19:28.120157 16096 layer_factory.hpp:77] Creating layer conv4
I0329 22:19:28.120173 16096 net.cpp:86] Creating Layer conv4
I0329 22:19:28.120179 16096 net.cpp:408] conv4 <- conv3
I0329 22:19:28.120190 16096 net.cpp:382] conv4 -> conv4
I0329 22:19:28.143251 16096 net.cpp:124] Setting up conv4
I0329 22:19:28.143306 16096 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0329 22:19:28.143311 16096 net.cpp:139] Memory required for data: 305964400
I0329 22:19:28.143326 16096 layer_factory.hpp:77] Creating layer relu4
I0329 22:19:28.143342 16096 net.cpp:86] Creating Layer relu4
I0329 22:19:28.143349 16096 net.cpp:408] relu4 <- conv4
I0329 22:19:28.143360 16096 net.cpp:369] relu4 -> conv4 (in-place)
I0329 22:19:28.143599 16096 net.cpp:124] Setting up relu4
I0329 22:19:28.143613 16096 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0329 22:19:28.143620 16096 net.cpp:139] Memory required for data: 318943600
I0329 22:19:28.143625 16096 layer_factory.hpp:77] Creating layer conv5
I0329 22:19:28.143643 16096 net.cpp:86] Creating Layer conv5
I0329 22:19:28.143649 16096 net.cpp:408] conv5 <- conv4
I0329 22:19:28.143661 16096 net.cpp:382] conv5 -> conv5
I0329 22:19:28.153527 16096 net.cpp:124] Setting up conv5
I0329 22:19:28.153576 16096 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0329 22:19:28.153583 16096 net.cpp:139] Memory required for data: 327596400
I0329 22:19:28.153604 16096 layer_factory.hpp:77] Creating layer relu5
I0329 22:19:28.153620 16096 net.cpp:86] Creating Layer relu5
I0329 22:19:28.153628 16096 net.cpp:408] relu5 <- conv5
I0329 22:19:28.153640 16096 net.cpp:369] relu5 -> conv5 (in-place)
I0329 22:19:28.153865 16096 net.cpp:124] Setting up relu5
I0329 22:19:28.153879 16096 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0329 22:19:28.153885 16096 net.cpp:139] Memory required for data: 336249200
I0329 22:19:28.153892 16096 layer_factory.hpp:77] Creating layer pool5
I0329 22:19:28.153906 16096 net.cpp:86] Creating Layer pool5
I0329 22:19:28.153913 16096 net.cpp:408] pool5 <- conv5
I0329 22:19:28.153921 16096 net.cpp:382] pool5 -> pool5
I0329 22:19:28.154002 16096 net.cpp:124] Setting up pool5
I0329 22:19:28.154027 16096 net.cpp:131] Top shape: 50 256 6 6 (460800)
I0329 22:19:28.154033 16096 net.cpp:139] Memory required for data: 338092400
I0329 22:19:28.154039 16096 layer_factory.hpp:77] Creating layer fc6
I0329 22:19:28.154052 16096 net.cpp:86] Creating Layer fc6
I0329 22:19:28.154057 16096 net.cpp:408] fc6 <- pool5
I0329 22:19:28.154067 16096 net.cpp:382] fc6 -> fc6
I0329 22:19:28.747632 16096 net.cpp:124] Setting up fc6
I0329 22:19:28.747681 16096 net.cpp:131] Top shape: 50 4096 (204800)
I0329 22:19:28.747689 16096 net.cpp:139] Memory required for data: 338911600
I0329 22:19:28.747706 16096 layer_factory.hpp:77] Creating layer relu6
I0329 22:19:28.747721 16096 net.cpp:86] Creating Layer relu6
I0329 22:19:28.747730 16096 net.cpp:408] relu6 <- fc6
I0329 22:19:28.747740 16096 net.cpp:369] relu6 -> fc6 (in-place)
I0329 22:19:28.767170 16096 net.cpp:124] Setting up relu6
I0329 22:19:28.767190 16096 net.cpp:131] Top shape: 50 4096 (204800)
I0329 22:19:28.767196 16096 net.cpp:139] Memory required for data: 339730800
I0329 22:19:28.767202 16096 layer_factory.hpp:77] Creating layer drop6
I0329 22:19:28.767215 16096 net.cpp:86] Creating Layer drop6
I0329 22:19:28.767221 16096 net.cpp:408] drop6 <- fc6
I0329 22:19:28.767231 16096 net.cpp:369] drop6 -> fc6 (in-place)
I0329 22:19:28.767277 16096 net.cpp:124] Setting up drop6
I0329 22:19:28.767288 16096 net.cpp:131] Top shape: 50 4096 (204800)
I0329 22:19:28.767294 16096 net.cpp:139] Memory required for data: 340550000
I0329 22:19:28.767299 16096 layer_factory.hpp:77] Creating layer fc7
I0329 22:19:28.767312 16096 net.cpp:86] Creating Layer fc7
I0329 22:19:28.767318 16096 net.cpp:408] fc7 <- fc6
I0329 22:19:28.767328 16096 net.cpp:382] fc7 -> fc7
I0329 22:19:29.029902 16096 net.cpp:124] Setting up fc7
I0329 22:19:29.029952 16096 net.cpp:131] Top shape: 50 4096 (204800)
I0329 22:19:29.029958 16096 net.cpp:139] Memory required for data: 341369200
I0329 22:19:29.029974 16096 layer_factory.hpp:77] Creating layer relu7
I0329 22:19:29.029989 16096 net.cpp:86] Creating Layer relu7
I0329 22:19:29.029996 16096 net.cpp:408] relu7 <- fc7
I0329 22:19:29.030007 16096 net.cpp:369] relu7 -> fc7 (in-place)
I0329 22:19:29.030300 16096 net.cpp:124] Setting up relu7
I0329 22:19:29.030315 16096 net.cpp:131] Top shape: 50 4096 (204800)
I0329 22:19:29.030320 16096 net.cpp:139] Memory required for data: 342188400
I0329 22:19:29.030326 16096 layer_factory.hpp:77] Creating layer drop7
I0329 22:19:29.030338 16096 net.cpp:86] Creating Layer drop7
I0329 22:19:29.030344 16096 net.cpp:408] drop7 <- fc7
I0329 22:19:29.030352 16096 net.cpp:369] drop7 -> fc7 (in-place)
I0329 22:19:29.030393 16096 net.cpp:124] Setting up drop7
I0329 22:19:29.030412 16096 net.cpp:131] Top shape: 50 4096 (204800)
I0329 22:19:29.030418 16096 net.cpp:139] Memory required for data: 343007600
I0329 22:19:29.030424 16096 layer_factory.hpp:77] Creating layer fc8
I0329 22:19:29.030436 16096 net.cpp:86] Creating Layer fc8
I0329 22:19:29.030442 16096 net.cpp:408] fc8 <- fc7
I0329 22:19:29.030452 16096 net.cpp:382] fc8 -> fc8
I0329 22:19:29.094889 16096 net.cpp:124] Setting up fc8
I0329 22:19:29.094941 16096 net.cpp:131] Top shape: 50 1000 (50000)
I0329 22:19:29.094947 16096 net.cpp:139] Memory required for data: 343207600
I0329 22:19:29.094964 16096 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0329 22:19:29.094977 16096 net.cpp:86] Creating Layer fc8_fc8_0_split
I0329 22:19:29.094985 16096 net.cpp:408] fc8_fc8_0_split <- fc8
I0329 22:19:29.094997 16096 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0329 22:19:29.095011 16096 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0329 22:19:29.095070 16096 net.cpp:124] Setting up fc8_fc8_0_split
I0329 22:19:29.095082 16096 net.cpp:131] Top shape: 50 1000 (50000)
I0329 22:19:29.095088 16096 net.cpp:131] Top shape: 50 1000 (50000)
I0329 22:19:29.095093 16096 net.cpp:139] Memory required for data: 343607600
I0329 22:19:29.095098 16096 layer_factory.hpp:77] Creating layer accuracy
I0329 22:19:29.095109 16096 net.cpp:86] Creating Layer accuracy
I0329 22:19:29.095130 16096 net.cpp:408] accuracy <- fc8_fc8_0_split_0
I0329 22:19:29.095151 16096 net.cpp:408] accuracy <- label_data_1_split_0
I0329 22:19:29.095161 16096 net.cpp:382] accuracy -> accuracy
I0329 22:19:29.095175 16096 net.cpp:124] Setting up accuracy
I0329 22:19:29.095183 16096 net.cpp:131] Top shape: (1)
I0329 22:19:29.095188 16096 net.cpp:139] Memory required for data: 343607604
I0329 22:19:29.095193 16096 layer_factory.hpp:77] Creating layer loss
I0329 22:19:29.095202 16096 net.cpp:86] Creating Layer loss
I0329 22:19:29.095208 16096 net.cpp:408] loss <- fc8_fc8_0_split_1
I0329 22:19:29.095216 16096 net.cpp:408] loss <- label_data_1_split_1
I0329 22:19:29.095223 16096 net.cpp:382] loss -> loss
I0329 22:19:29.095235 16096 layer_factory.hpp:77] Creating layer loss
I0329 22:19:29.095963 16096 net.cpp:124] Setting up loss
I0329 22:19:29.095978 16096 net.cpp:131] Top shape: (1)
I0329 22:19:29.095984 16096 net.cpp:134]     with loss weight 1
I0329 22:19:29.096004 16096 net.cpp:139] Memory required for data: 343607608
I0329 22:19:29.096009 16096 net.cpp:200] loss needs backward computation.
I0329 22:19:29.096017 16096 net.cpp:202] accuracy does not need backward computation.
I0329 22:19:29.096024 16096 net.cpp:200] fc8_fc8_0_split needs backward computation.
I0329 22:19:29.096030 16096 net.cpp:200] fc8 needs backward computation.
I0329 22:19:29.096036 16096 net.cpp:200] drop7 needs backward computation.
I0329 22:19:29.096042 16096 net.cpp:200] relu7 needs backward computation.
I0329 22:19:29.096047 16096 net.cpp:200] fc7 needs backward computation.
I0329 22:19:29.096053 16096 net.cpp:200] drop6 needs backward computation.
I0329 22:19:29.096060 16096 net.cpp:200] relu6 needs backward computation.
I0329 22:19:29.096065 16096 net.cpp:200] fc6 needs backward computation.
I0329 22:19:29.096071 16096 net.cpp:200] pool5 needs backward computation.
I0329 22:19:29.096076 16096 net.cpp:200] relu5 needs backward computation.
I0329 22:19:29.096082 16096 net.cpp:200] conv5 needs backward computation.
I0329 22:19:29.096088 16096 net.cpp:200] relu4 needs backward computation.
I0329 22:19:29.096093 16096 net.cpp:200] conv4 needs backward computation.
I0329 22:19:29.096099 16096 net.cpp:200] relu3 needs backward computation.
I0329 22:19:29.096105 16096 net.cpp:200] conv3 needs backward computation.
I0329 22:19:29.096112 16096 net.cpp:200] norm2 needs backward computation.
I0329 22:19:29.096117 16096 net.cpp:200] pool2 needs backward computation.
I0329 22:19:29.096123 16096 net.cpp:200] relu2 needs backward computation.
I0329 22:19:29.096128 16096 net.cpp:200] conv2 needs backward computation.
I0329 22:19:29.096134 16096 net.cpp:200] norm1 needs backward computation.
I0329 22:19:29.096140 16096 net.cpp:200] pool1 needs backward computation.
I0329 22:19:29.096146 16096 net.cpp:200] relu1 needs backward computation.
I0329 22:19:29.096153 16096 net.cpp:200] conv1 needs backward computation.
I0329 22:19:29.096158 16096 net.cpp:202] label_data_1_split does not need backward computation.
I0329 22:19:29.096165 16096 net.cpp:202] data does not need backward computation.
I0329 22:19:29.096170 16096 net.cpp:244] This network produces output accuracy
I0329 22:19:29.096176 16096 net.cpp:244] This network produces output loss
I0329 22:19:29.096199 16096 net.cpp:257] Network initialization done.
I0329 22:19:29.096302 16096 solver.cpp:56] Solver scaffolding done.
I0329 22:19:29.097057 16096 caffe.cpp:248] Starting Optimization
I0329 22:19:29.097069 16096 solver.cpp:273] Solving CaffeNet
I0329 22:19:29.097074 16096 solver.cpp:274] Learning Rate Policy: fixed
I0329 22:19:29.099464 16096 solver.cpp:331] Iteration 0, Testing net (#0)
I0329 22:19:36.067088 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 22:19:36.067157 16096 solver.cpp:398]     Test net output #1: loss = 7.14966 (* 1 = 7.14966 loss)
I0329 22:19:37.046275 16096 solver.cpp:219] Iteration 0 (5.72607e+17 iter/s, 7.94881s/20 iters), loss = 7.47207
I0329 22:19:37.046353 16096 solver.cpp:238]     Train net output #0: loss = 7.47207 (* 1 = 7.47207 loss)
I0329 22:19:37.046378 16096 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I0329 22:19:55.551499 16096 blocking_queue.cpp:49] Waiting for data
I0329 22:19:57.204254 16096 solver.cpp:219] Iteration 20 (0.992184 iter/s, 20.1576s/20 iters), loss = 6.90305
I0329 22:19:57.227877 16096 solver.cpp:238]     Train net output #0: loss = 6.90305 (* 1 = 6.90305 loss)
I0329 22:19:57.227908 16096 sgd_solver.cpp:105] Iteration 20, lr = 0.1
I0329 22:20:16.893105 16096 solver.cpp:219] Iteration 40 (1.01704 iter/s, 19.6649s/20 iters), loss = 6.90521
I0329 22:20:16.916731 16096 solver.cpp:238]     Train net output #0: loss = 6.90521 (* 1 = 6.90521 loss)
I0329 22:20:16.916764 16096 sgd_solver.cpp:105] Iteration 40, lr = 0.1
I0329 22:20:36.594986 16096 solver.cpp:219] Iteration 60 (1.01637 iter/s, 19.6779s/20 iters), loss = 6.90011
I0329 22:20:36.618608 16096 solver.cpp:238]     Train net output #0: loss = 6.90011 (* 1 = 6.90011 loss)
I0329 22:20:36.618640 16096 sgd_solver.cpp:105] Iteration 60, lr = 0.1
I0329 22:20:56.293172 16096 solver.cpp:219] Iteration 80 (1.01656 iter/s, 19.6742s/20 iters), loss = 6.90479
I0329 22:20:56.316786 16096 solver.cpp:238]     Train net output #0: loss = 6.90479 (* 1 = 6.90479 loss)
I0329 22:20:56.316817 16096 sgd_solver.cpp:105] Iteration 80, lr = 0.1
I0329 22:21:14.364929 16096 solver.cpp:331] Iteration 100, Testing net (#0)
I0329 22:21:38.921078 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0016
I0329 22:21:38.921175 16096 solver.cpp:398]     Test net output #1: loss = 6.91175 (* 1 = 6.91175 loss)
I0329 22:21:39.891589 16096 solver.cpp:219] Iteration 100 (0.45899 iter/s, 43.574s/20 iters), loss = 6.89866
I0329 22:21:39.891677 16096 solver.cpp:238]     Train net output #0: loss = 6.89866 (* 1 = 6.89866 loss)
I0329 22:21:39.891690 16096 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I0329 22:21:59.532392 16096 solver.cpp:219] Iteration 120 (1.01831 iter/s, 19.6403s/20 iters), loss = 6.89819
I0329 22:21:59.556017 16096 solver.cpp:238]     Train net output #0: loss = 6.89819 (* 1 = 6.89819 loss)
I0329 22:21:59.556046 16096 sgd_solver.cpp:105] Iteration 120, lr = 0.1
I0329 22:22:19.250334 16096 solver.cpp:219] Iteration 140 (1.01554 iter/s, 19.6939s/20 iters), loss = 6.88795
I0329 22:22:19.273948 16096 solver.cpp:238]     Train net output #0: loss = 6.88795 (* 1 = 6.88795 loss)
I0329 22:22:19.273980 16096 sgd_solver.cpp:105] Iteration 140, lr = 0.1
I0329 22:22:38.958370 16096 solver.cpp:219] Iteration 160 (1.01605 iter/s, 19.684s/20 iters), loss = 6.88482
I0329 22:22:38.981987 16096 solver.cpp:238]     Train net output #0: loss = 6.88482 (* 1 = 6.88482 loss)
I0329 22:22:38.982017 16096 sgd_solver.cpp:105] Iteration 160, lr = 0.1
I0329 22:22:58.634989 16096 solver.cpp:219] Iteration 180 (1.01768 iter/s, 19.6526s/20 iters), loss = 6.88839
I0329 22:22:58.658625 16096 solver.cpp:238]     Train net output #0: loss = 6.88839 (* 1 = 6.88839 loss)
I0329 22:22:58.658658 16096 sgd_solver.cpp:105] Iteration 180, lr = 0.1
I0329 22:23:16.686061 16096 solver.cpp:331] Iteration 200, Testing net (#0)
I0329 22:23:33.798653 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0012
I0329 22:23:33.798741 16096 solver.cpp:398]     Test net output #1: loss = 6.91397 (* 1 = 6.91397 loss)
I0329 22:23:34.765951 16096 solver.cpp:219] Iteration 200 (0.553914 iter/s, 36.1067s/20 iters), loss = 6.88434
I0329 22:23:34.766027 16096 solver.cpp:238]     Train net output #0: loss = 6.88434 (* 1 = 6.88434 loss)
I0329 22:23:34.766041 16096 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I0329 22:23:54.468322 16096 solver.cpp:219] Iteration 220 (1.01513 iter/s, 19.702s/20 iters), loss = 6.87669
I0329 22:23:54.491951 16096 solver.cpp:238]     Train net output #0: loss = 6.87669 (* 1 = 6.87669 loss)
I0329 22:23:54.491981 16096 sgd_solver.cpp:105] Iteration 220, lr = 0.1
I0329 22:24:14.156158 16096 solver.cpp:219] Iteration 240 (1.01709 iter/s, 19.6639s/20 iters), loss = 6.87753
I0329 22:24:14.179776 16096 solver.cpp:238]     Train net output #0: loss = 6.87753 (* 1 = 6.87753 loss)
I0329 22:24:14.179810 16096 sgd_solver.cpp:105] Iteration 240, lr = 0.1
I0329 22:24:34.155510 16096 solver.cpp:219] Iteration 260 (1.00123 iter/s, 19.9754s/20 iters), loss = 6.87381
I0329 22:24:34.179129 16096 solver.cpp:238]     Train net output #0: loss = 6.87381 (* 1 = 6.87381 loss)
I0329 22:24:34.179157 16096 sgd_solver.cpp:105] Iteration 260, lr = 0.1
I0329 22:24:53.851049 16096 solver.cpp:219] Iteration 280 (1.01669 iter/s, 19.6716s/20 iters), loss = 6.88004
I0329 22:24:53.874675 16096 solver.cpp:238]     Train net output #0: loss = 6.88004 (* 1 = 6.88004 loss)
I0329 22:24:53.874707 16096 sgd_solver.cpp:105] Iteration 280, lr = 0.1
I0329 22:25:11.906358 16096 solver.cpp:331] Iteration 300, Testing net (#0)
I0329 22:25:39.813657 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0014
I0329 22:25:39.813760 16096 solver.cpp:398]     Test net output #1: loss = 6.91971 (* 1 = 6.91971 loss)
I0329 22:25:40.780038 16096 solver.cpp:219] Iteration 300 (0.426397 iter/s, 46.9047s/20 iters), loss = 6.87412
I0329 22:25:40.780127 16096 solver.cpp:238]     Train net output #0: loss = 6.87412 (* 1 = 6.87412 loss)
I0329 22:25:40.780138 16096 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I0329 22:26:00.771973 16096 solver.cpp:219] Iteration 320 (1.00042 iter/s, 19.9915s/20 iters), loss = 6.87012
I0329 22:26:00.795588 16096 solver.cpp:238]     Train net output #0: loss = 6.87012 (* 1 = 6.87012 loss)
I0329 22:26:00.795617 16096 sgd_solver.cpp:105] Iteration 320, lr = 0.1
I0329 22:26:20.459851 16096 solver.cpp:219] Iteration 340 (1.01709 iter/s, 19.664s/20 iters), loss = 6.87877
I0329 22:26:20.483469 16096 solver.cpp:238]     Train net output #0: loss = 6.87877 (* 1 = 6.87877 loss)
I0329 22:26:20.483500 16096 sgd_solver.cpp:105] Iteration 340, lr = 0.1
I0329 22:26:40.143028 16096 solver.cpp:219] Iteration 360 (1.01733 iter/s, 19.6593s/20 iters), loss = 6.87929
I0329 22:26:40.166673 16096 solver.cpp:238]     Train net output #0: loss = 6.87929 (* 1 = 6.87929 loss)
I0329 22:26:40.166702 16096 sgd_solver.cpp:105] Iteration 360, lr = 0.1
I0329 22:27:00.167132 16096 solver.cpp:219] Iteration 380 (0.999992 iter/s, 20.0002s/20 iters), loss = 6.86678
I0329 22:27:00.190721 16096 solver.cpp:238]     Train net output #0: loss = 6.86678 (* 1 = 6.86678 loss)
I0329 22:27:00.190743 16096 sgd_solver.cpp:105] Iteration 380, lr = 0.1
I0329 22:27:18.401520 16096 solver.cpp:331] Iteration 400, Testing net (#0)
I0329 22:27:42.359937 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0002
I0329 22:27:42.360013 16096 solver.cpp:398]     Test net output #1: loss = 6.92149 (* 1 = 6.92149 loss)
I0329 22:27:43.326768 16096 solver.cpp:219] Iteration 400 (0.463656 iter/s, 43.1354s/20 iters), loss = 6.86838
I0329 22:27:43.326844 16096 solver.cpp:238]     Train net output #0: loss = 6.86838 (* 1 = 6.86838 loss)
I0329 22:27:43.326858 16096 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I0329 22:28:02.992604 16096 solver.cpp:219] Iteration 420 (1.01701 iter/s, 19.6655s/20 iters), loss = 6.86612
I0329 22:28:03.016219 16096 solver.cpp:238]     Train net output #0: loss = 6.86612 (* 1 = 6.86612 loss)
I0329 22:28:03.016250 16096 sgd_solver.cpp:105] Iteration 420, lr = 0.1
I0329 22:28:22.686884 16096 solver.cpp:219] Iteration 440 (1.01676 iter/s, 19.6704s/20 iters), loss = 6.86712
I0329 22:28:22.710505 16096 solver.cpp:238]     Train net output #0: loss = 6.86712 (* 1 = 6.86712 loss)
I0329 22:28:22.710535 16096 sgd_solver.cpp:105] Iteration 440, lr = 0.1
I0329 22:28:42.398599 16096 solver.cpp:219] Iteration 460 (1.01586 iter/s, 19.6878s/20 iters), loss = 6.86273
I0329 22:28:42.422237 16096 solver.cpp:238]     Train net output #0: loss = 6.86273 (* 1 = 6.86273 loss)
I0329 22:28:42.422266 16096 sgd_solver.cpp:105] Iteration 460, lr = 0.1
I0329 22:29:02.221474 16096 solver.cpp:219] Iteration 480 (1.01016 iter/s, 19.7989s/20 iters), loss = 6.85764
I0329 22:29:02.245088 16096 solver.cpp:238]     Train net output #0: loss = 6.85764 (* 1 = 6.85764 loss)
I0329 22:29:02.245121 16096 sgd_solver.cpp:105] Iteration 480, lr = 0.1
I0329 22:29:20.285365 16096 solver.cpp:331] Iteration 500, Testing net (#0)
I0329 22:29:38.233976 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0014
I0329 22:29:38.234079 16096 solver.cpp:398]     Test net output #1: loss = 6.92797 (* 1 = 6.92797 loss)
I0329 22:29:39.203420 16096 solver.cpp:219] Iteration 500 (0.541158 iter/s, 36.9578s/20 iters), loss = 6.86037
I0329 22:29:39.203506 16096 solver.cpp:238]     Train net output #0: loss = 6.86037 (* 1 = 6.86037 loss)
I0329 22:29:39.203519 16096 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I0329 22:29:59.207546 16096 solver.cpp:219] Iteration 520 (0.999814 iter/s, 20.0037s/20 iters), loss = 6.86989
I0329 22:29:59.231178 16096 solver.cpp:238]     Train net output #0: loss = 6.86989 (* 1 = 6.86989 loss)
I0329 22:29:59.231206 16096 sgd_solver.cpp:105] Iteration 520, lr = 0.1
I0329 22:30:18.935904 16096 solver.cpp:219] Iteration 540 (1.015 iter/s, 19.7044s/20 iters), loss = 6.87201
I0329 22:30:18.959532 16096 solver.cpp:238]     Train net output #0: loss = 6.87201 (* 1 = 6.87201 loss)
I0329 22:30:18.959563 16096 sgd_solver.cpp:105] Iteration 540, lr = 0.1
I0329 22:30:38.616472 16096 solver.cpp:219] Iteration 560 (1.01747 iter/s, 19.6566s/20 iters), loss = 6.86744
I0329 22:30:38.640094 16096 solver.cpp:238]     Train net output #0: loss = 6.86744 (* 1 = 6.86744 loss)
I0329 22:30:38.640123 16096 sgd_solver.cpp:105] Iteration 560, lr = 0.1
I0329 22:30:58.339208 16096 solver.cpp:219] Iteration 580 (1.01529 iter/s, 19.6988s/20 iters), loss = 6.86251
I0329 22:30:58.362826 16096 solver.cpp:238]     Train net output #0: loss = 6.86251 (* 1 = 6.86251 loss)
I0329 22:30:58.362860 16096 sgd_solver.cpp:105] Iteration 580, lr = 0.1
I0329 22:31:16.397258 16096 solver.cpp:331] Iteration 600, Testing net (#0)
I0329 22:31:30.019371 16096 blocking_queue.cpp:49] Waiting for data
I0329 22:31:32.176645 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 22:31:32.176707 16096 solver.cpp:398]     Test net output #1: loss = 6.92967 (* 1 = 6.92967 loss)
I0329 22:31:33.140295 16096 solver.cpp:219] Iteration 600 (0.575094 iter/s, 34.7769s/20 iters), loss = 6.86115
I0329 22:31:33.140382 16096 solver.cpp:238]     Train net output #0: loss = 6.86115 (* 1 = 6.86115 loss)
I0329 22:31:33.140393 16096 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I0329 22:31:52.770133 16096 solver.cpp:219] Iteration 620 (1.01888 iter/s, 19.6294s/20 iters), loss = 6.87319
I0329 22:31:52.793751 16096 solver.cpp:238]     Train net output #0: loss = 6.87319 (* 1 = 6.87319 loss)
I0329 22:31:52.793782 16096 sgd_solver.cpp:105] Iteration 620, lr = 0.1
I0329 22:32:12.455781 16096 solver.cpp:219] Iteration 640 (1.0172 iter/s, 19.6617s/20 iters), loss = 6.85564
I0329 22:32:12.479401 16096 solver.cpp:238]     Train net output #0: loss = 6.85564 (* 1 = 6.85564 loss)
I0329 22:32:12.479435 16096 sgd_solver.cpp:105] Iteration 640, lr = 0.1
I0329 22:32:32.152439 16096 solver.cpp:219] Iteration 660 (1.01664 iter/s, 19.6727s/20 iters), loss = 6.85448
I0329 22:32:32.176055 16096 solver.cpp:238]     Train net output #0: loss = 6.85448 (* 1 = 6.85448 loss)
I0329 22:32:32.176082 16096 sgd_solver.cpp:105] Iteration 660, lr = 0.1
I0329 22:32:51.838194 16096 solver.cpp:219] Iteration 680 (1.0172 iter/s, 19.6618s/20 iters), loss = 6.84946
I0329 22:32:51.861806 16096 solver.cpp:238]     Train net output #0: loss = 6.84946 (* 1 = 6.84946 loss)
I0329 22:32:51.861837 16096 sgd_solver.cpp:105] Iteration 680, lr = 0.1
I0329 22:33:09.879606 16096 solver.cpp:331] Iteration 700, Testing net (#0)
I0329 22:33:26.154647 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0014
I0329 22:33:26.154721 16096 solver.cpp:398]     Test net output #1: loss = 6.93265 (* 1 = 6.93265 loss)
I0329 22:33:27.120951 16096 solver.cpp:219] Iteration 700 (0.567238 iter/s, 35.2586s/20 iters), loss = 6.85842
I0329 22:33:27.121024 16096 solver.cpp:238]     Train net output #0: loss = 6.85842 (* 1 = 6.85842 loss)
I0329 22:33:27.121037 16096 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I0329 22:33:46.791695 16096 solver.cpp:219] Iteration 720 (1.01676 iter/s, 19.6703s/20 iters), loss = 6.85326
I0329 22:33:46.815312 16096 solver.cpp:238]     Train net output #0: loss = 6.85326 (* 1 = 6.85326 loss)
I0329 22:33:46.815353 16096 sgd_solver.cpp:105] Iteration 720, lr = 0.1
I0329 22:34:06.549343 16096 solver.cpp:219] Iteration 740 (1.01349 iter/s, 19.7337s/20 iters), loss = 6.85953
I0329 22:34:06.572953 16096 solver.cpp:238]     Train net output #0: loss = 6.85953 (* 1 = 6.85953 loss)
I0329 22:34:06.572984 16096 sgd_solver.cpp:105] Iteration 740, lr = 0.1
I0329 22:34:26.354970 16096 solver.cpp:219] Iteration 760 (1.01104 iter/s, 19.7817s/20 iters), loss = 6.84767
I0329 22:34:26.378585 16096 solver.cpp:238]     Train net output #0: loss = 6.84767 (* 1 = 6.84767 loss)
I0329 22:34:26.378614 16096 sgd_solver.cpp:105] Iteration 760, lr = 0.1
I0329 22:34:46.148123 16096 solver.cpp:219] Iteration 780 (1.01167 iter/s, 19.7692s/20 iters), loss = 6.85127
I0329 22:34:46.171748 16096 solver.cpp:238]     Train net output #0: loss = 6.85127 (* 1 = 6.85127 loss)
I0329 22:34:46.171782 16096 sgd_solver.cpp:105] Iteration 780, lr = 0.1
I0329 22:35:04.247831 16096 solver.cpp:331] Iteration 800, Testing net (#0)
I0329 22:35:22.788882 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0012
I0329 22:35:22.789000 16096 solver.cpp:398]     Test net output #1: loss = 6.94385 (* 1 = 6.94385 loss)
I0329 22:35:23.756564 16096 solver.cpp:219] Iteration 800 (0.532138 iter/s, 37.5842s/20 iters), loss = 6.864
I0329 22:35:23.756654 16096 solver.cpp:238]     Train net output #0: loss = 6.864 (* 1 = 6.864 loss)
I0329 22:35:23.756666 16096 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I0329 22:35:43.730327 16096 solver.cpp:219] Iteration 820 (1.00133 iter/s, 19.9734s/20 iters), loss = 6.86238
I0329 22:35:43.753944 16096 solver.cpp:238]     Train net output #0: loss = 6.86238 (* 1 = 6.86238 loss)
I0329 22:35:43.753975 16096 sgd_solver.cpp:105] Iteration 820, lr = 0.1
I0329 22:36:03.514709 16096 solver.cpp:219] Iteration 840 (1.01212 iter/s, 19.7605s/20 iters), loss = 6.83951
I0329 22:36:03.538324 16096 solver.cpp:238]     Train net output #0: loss = 6.83951 (* 1 = 6.83951 loss)
I0329 22:36:03.538357 16096 sgd_solver.cpp:105] Iteration 840, lr = 0.1
I0329 22:36:23.202610 16096 solver.cpp:219] Iteration 860 (1.01709 iter/s, 19.664s/20 iters), loss = 6.84608
I0329 22:36:23.226234 16096 solver.cpp:238]     Train net output #0: loss = 6.84608 (* 1 = 6.84608 loss)
I0329 22:36:23.226263 16096 sgd_solver.cpp:105] Iteration 860, lr = 0.1
I0329 22:36:42.902796 16096 solver.cpp:219] Iteration 880 (1.01645 iter/s, 19.6763s/20 iters), loss = 6.83635
I0329 22:36:42.926429 16096 solver.cpp:238]     Train net output #0: loss = 6.83635 (* 1 = 6.83635 loss)
I0329 22:36:42.926458 16096 sgd_solver.cpp:105] Iteration 880, lr = 0.1
I0329 22:37:00.955175 16096 solver.cpp:331] Iteration 900, Testing net (#0)
I0329 22:37:20.976989 16109 data_layer.cpp:73] Restarting data prefetching from start.
I0329 22:37:21.057183 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0006
I0329 22:37:21.057260 16096 solver.cpp:398]     Test net output #1: loss = 6.95435 (* 1 = 6.95435 loss)
I0329 22:37:22.024447 16096 solver.cpp:219] Iteration 900 (0.511543 iter/s, 39.0974s/20 iters), loss = 6.84832
I0329 22:37:22.024530 16096 solver.cpp:238]     Train net output #0: loss = 6.84832 (* 1 = 6.84832 loss)
I0329 22:37:22.024544 16096 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I0329 22:37:41.793747 16096 solver.cpp:219] Iteration 920 (1.01169 iter/s, 19.7689s/20 iters), loss = 6.85844
I0329 22:37:41.793964 16096 solver.cpp:238]     Train net output #0: loss = 6.85844 (* 1 = 6.85844 loss)
I0329 22:37:41.793982 16096 sgd_solver.cpp:105] Iteration 920, lr = 0.1
I0329 22:38:01.505339 16096 solver.cpp:219] Iteration 940 (1.01467 iter/s, 19.7108s/20 iters), loss = 6.8521
I0329 22:38:01.528962 16096 solver.cpp:238]     Train net output #0: loss = 6.8521 (* 1 = 6.8521 loss)
I0329 22:38:01.528995 16096 sgd_solver.cpp:105] Iteration 940, lr = 0.1
I0329 22:38:21.215191 16096 solver.cpp:219] Iteration 960 (1.01595 iter/s, 19.6859s/20 iters), loss = 6.84055
I0329 22:38:21.238807 16096 solver.cpp:238]     Train net output #0: loss = 6.84055 (* 1 = 6.84055 loss)
I0329 22:38:21.238848 16096 sgd_solver.cpp:105] Iteration 960, lr = 0.1
I0329 22:38:41.019655 16096 solver.cpp:219] Iteration 980 (1.01109 iter/s, 19.7805s/20 iters), loss = 6.85395
I0329 22:38:41.043269 16096 solver.cpp:238]     Train net output #0: loss = 6.85395 (* 1 = 6.85395 loss)
I0329 22:38:41.043300 16096 sgd_solver.cpp:105] Iteration 980, lr = 0.1
I0329 22:38:59.275249 16096 solver.cpp:331] Iteration 1000, Testing net (#0)
I0329 22:39:06.920279 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0016
I0329 22:39:06.920354 16096 solver.cpp:398]     Test net output #1: loss = 6.94855 (* 1 = 6.94855 loss)
I0329 22:39:07.885116 16096 solver.cpp:219] Iteration 1000 (0.745117 iter/s, 26.8414s/20 iters), loss = 6.85078
I0329 22:39:07.885203 16096 solver.cpp:238]     Train net output #0: loss = 6.85078 (* 1 = 6.85078 loss)
I0329 22:39:07.885220 16096 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I0329 22:39:27.801189 16096 solver.cpp:219] Iteration 1020 (1.00424 iter/s, 19.9156s/20 iters), loss = 6.85104
I0329 22:39:27.824806 16096 solver.cpp:238]     Train net output #0: loss = 6.85104 (* 1 = 6.85104 loss)
I0329 22:39:27.824837 16096 sgd_solver.cpp:105] Iteration 1020, lr = 0.1
I0329 22:39:47.496379 16096 solver.cpp:219] Iteration 1040 (1.01671 iter/s, 19.6713s/20 iters), loss = 6.8498
I0329 22:39:47.520004 16096 solver.cpp:238]     Train net output #0: loss = 6.8498 (* 1 = 6.8498 loss)
I0329 22:39:47.520033 16096 sgd_solver.cpp:105] Iteration 1040, lr = 0.1
I0329 22:40:07.202620 16096 solver.cpp:219] Iteration 1060 (1.01614 iter/s, 19.6823s/20 iters), loss = 6.85377
I0329 22:40:07.226240 16096 solver.cpp:238]     Train net output #0: loss = 6.85377 (* 1 = 6.85377 loss)
I0329 22:40:07.226271 16096 sgd_solver.cpp:105] Iteration 1060, lr = 0.1
I0329 22:40:26.914468 16096 solver.cpp:219] Iteration 1080 (1.01585 iter/s, 19.6879s/20 iters), loss = 6.85131
I0329 22:40:26.938086 16096 solver.cpp:238]     Train net output #0: loss = 6.85131 (* 1 = 6.85131 loss)
I0329 22:40:26.938117 16096 sgd_solver.cpp:105] Iteration 1080, lr = 0.1
I0329 22:40:44.976724 16096 solver.cpp:331] Iteration 1100, Testing net (#0)
I0329 22:41:08.936601 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 22:41:08.936858 16096 solver.cpp:398]     Test net output #1: loss = 6.95717 (* 1 = 6.95717 loss)
I0329 22:41:09.906891 16096 solver.cpp:219] Iteration 1100 (0.465461 iter/s, 42.9681s/20 iters), loss = 6.83351
I0329 22:41:09.906982 16096 solver.cpp:238]     Train net output #0: loss = 6.83351 (* 1 = 6.83351 loss)
I0329 22:41:09.906996 16096 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I0329 22:41:29.607554 16096 solver.cpp:219] Iteration 1120 (1.01522 iter/s, 19.7003s/20 iters), loss = 6.83185
I0329 22:41:29.631163 16096 solver.cpp:238]     Train net output #0: loss = 6.83185 (* 1 = 6.83185 loss)
I0329 22:41:29.631196 16096 sgd_solver.cpp:105] Iteration 1120, lr = 0.1
I0329 22:41:49.344684 16096 solver.cpp:219] Iteration 1140 (1.01455 iter/s, 19.7132s/20 iters), loss = 6.82147
I0329 22:41:49.368299 16096 solver.cpp:238]     Train net output #0: loss = 6.82147 (* 1 = 6.82147 loss)
I0329 22:41:49.368330 16096 sgd_solver.cpp:105] Iteration 1140, lr = 0.1
I0329 22:42:09.058107 16096 solver.cpp:219] Iteration 1160 (1.01577 iter/s, 19.6895s/20 iters), loss = 6.83843
I0329 22:42:09.081727 16096 solver.cpp:238]     Train net output #0: loss = 6.83843 (* 1 = 6.83843 loss)
I0329 22:42:09.081761 16096 sgd_solver.cpp:105] Iteration 1160, lr = 0.1
I0329 22:42:28.774859 16096 solver.cpp:219] Iteration 1180 (1.0156 iter/s, 19.6928s/20 iters), loss = 6.87995
I0329 22:42:28.798475 16096 solver.cpp:238]     Train net output #0: loss = 6.87995 (* 1 = 6.87995 loss)
I0329 22:42:28.798502 16096 sgd_solver.cpp:105] Iteration 1180, lr = 0.1
I0329 22:42:46.839725 16096 solver.cpp:331] Iteration 1200, Testing net (#0)
I0329 22:43:02.926890 16096 blocking_queue.cpp:49] Waiting for data
I0329 22:43:04.162644 16096 solver.cpp:398]     Test net output #0: accuracy = 0.001
I0329 22:43:04.162719 16096 solver.cpp:398]     Test net output #1: loss = 6.96384 (* 1 = 6.96384 loss)
I0329 22:43:05.133358 16096 solver.cpp:219] Iteration 1200 (0.550444 iter/s, 36.3343s/20 iters), loss = 6.84269
I0329 22:43:05.133446 16096 solver.cpp:238]     Train net output #0: loss = 6.84269 (* 1 = 6.84269 loss)
I0329 22:43:05.133458 16096 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I0329 22:43:24.846916 16096 solver.cpp:219] Iteration 1220 (1.01455 iter/s, 19.7131s/20 iters), loss = 6.83806
I0329 22:43:24.870538 16096 solver.cpp:238]     Train net output #0: loss = 6.83806 (* 1 = 6.83806 loss)
I0329 22:43:24.870571 16096 sgd_solver.cpp:105] Iteration 1220, lr = 0.1
I0329 22:43:44.572614 16096 solver.cpp:219] Iteration 1240 (1.01514 iter/s, 19.7018s/20 iters), loss = 6.86357
I0329 22:43:44.596230 16096 solver.cpp:238]     Train net output #0: loss = 6.86357 (* 1 = 6.86357 loss)
I0329 22:43:44.596257 16096 sgd_solver.cpp:105] Iteration 1240, lr = 0.1
I0329 22:44:04.322468 16096 solver.cpp:219] Iteration 1260 (1.01389 iter/s, 19.7259s/20 iters), loss = 6.82583
I0329 22:44:04.346081 16096 solver.cpp:238]     Train net output #0: loss = 6.82583 (* 1 = 6.82583 loss)
I0329 22:44:04.346112 16096 sgd_solver.cpp:105] Iteration 1260, lr = 0.1
I0329 22:44:24.049705 16096 solver.cpp:219] Iteration 1280 (1.01506 iter/s, 19.7033s/20 iters), loss = 6.83276
I0329 22:44:24.073330 16096 solver.cpp:238]     Train net output #0: loss = 6.83276 (* 1 = 6.83276 loss)
I0329 22:44:24.073360 16096 sgd_solver.cpp:105] Iteration 1280, lr = 0.1
I0329 22:44:42.114231 16096 solver.cpp:331] Iteration 1300, Testing net (#0)
I0329 22:45:03.820641 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0016
I0329 22:45:03.820907 16096 solver.cpp:398]     Test net output #1: loss = 6.94787 (* 1 = 6.94787 loss)
I0329 22:45:04.786118 16096 solver.cpp:219] Iteration 1300 (0.491254 iter/s, 40.7121s/20 iters), loss = 6.81706
I0329 22:45:04.786204 16096 solver.cpp:238]     Train net output #0: loss = 6.81706 (* 1 = 6.81706 loss)
I0329 22:45:04.786217 16096 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I0329 22:45:25.733001 16096 solver.cpp:219] Iteration 1320 (0.954815 iter/s, 20.9465s/20 iters), loss = 6.78611
I0329 22:45:25.756624 16096 solver.cpp:238]     Train net output #0: loss = 6.78611 (* 1 = 6.78611 loss)
I0329 22:45:25.756656 16096 sgd_solver.cpp:105] Iteration 1320, lr = 0.1
I0329 22:45:45.457953 16096 solver.cpp:219] Iteration 1340 (1.01518 iter/s, 19.701s/20 iters), loss = 6.8359
I0329 22:45:45.481578 16096 solver.cpp:238]     Train net output #0: loss = 6.8359 (* 1 = 6.8359 loss)
I0329 22:45:45.481609 16096 sgd_solver.cpp:105] Iteration 1340, lr = 0.1
I0329 22:46:05.178211 16096 solver.cpp:219] Iteration 1360 (1.01542 iter/s, 19.6963s/20 iters), loss = 6.8348
I0329 22:46:05.201825 16096 solver.cpp:238]     Train net output #0: loss = 6.8348 (* 1 = 6.8348 loss)
I0329 22:46:05.201855 16096 sgd_solver.cpp:105] Iteration 1360, lr = 0.1
I0329 22:46:24.890283 16096 solver.cpp:219] Iteration 1380 (1.01584 iter/s, 19.6881s/20 iters), loss = 6.76814
I0329 22:46:24.913898 16096 solver.cpp:238]     Train net output #0: loss = 6.76814 (* 1 = 6.76814 loss)
I0329 22:46:24.913928 16096 sgd_solver.cpp:105] Iteration 1380, lr = 0.1
I0329 22:46:42.940007 16096 solver.cpp:331] Iteration 1400, Testing net (#0)
I0329 22:47:06.492630 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0024
I0329 22:47:06.492735 16096 solver.cpp:398]     Test net output #1: loss = 6.92066 (* 1 = 6.92066 loss)
I0329 22:47:07.458941 16096 solver.cpp:219] Iteration 1400 (0.470097 iter/s, 42.5444s/20 iters), loss = 6.82538
I0329 22:47:07.459025 16096 solver.cpp:238]     Train net output #0: loss = 6.82538 (* 1 = 6.82538 loss)
I0329 22:47:07.459039 16096 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I0329 22:47:28.415696 16096 solver.cpp:219] Iteration 1420 (0.954365 iter/s, 20.9563s/20 iters), loss = 6.81159
I0329 22:47:28.439316 16096 solver.cpp:238]     Train net output #0: loss = 6.81159 (* 1 = 6.81159 loss)
I0329 22:47:28.439345 16096 sgd_solver.cpp:105] Iteration 1420, lr = 0.1
I0329 22:47:48.127360 16096 solver.cpp:219] Iteration 1440 (1.01586 iter/s, 19.6877s/20 iters), loss = 6.78261
I0329 22:47:48.150979 16096 solver.cpp:238]     Train net output #0: loss = 6.78261 (* 1 = 6.78261 loss)
I0329 22:47:48.151007 16096 sgd_solver.cpp:105] Iteration 1440, lr = 0.1
I0329 22:48:07.824755 16096 solver.cpp:219] Iteration 1460 (1.0166 iter/s, 19.6735s/20 iters), loss = 6.83267
I0329 22:48:07.848369 16096 solver.cpp:238]     Train net output #0: loss = 6.83267 (* 1 = 6.83267 loss)
I0329 22:48:07.848400 16096 sgd_solver.cpp:105] Iteration 1460, lr = 0.1
I0329 22:48:27.518157 16096 solver.cpp:219] Iteration 1480 (1.0168 iter/s, 19.6695s/20 iters), loss = 6.84984
I0329 22:48:27.541771 16096 solver.cpp:238]     Train net output #0: loss = 6.84984 (* 1 = 6.84984 loss)
I0329 22:48:27.541802 16096 sgd_solver.cpp:105] Iteration 1480, lr = 0.1
I0329 22:48:45.554558 16096 solver.cpp:331] Iteration 1500, Testing net (#0)
I0329 22:49:03.049610 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 22:49:03.049743 16096 solver.cpp:398]     Test net output #1: loss = 6.96958 (* 1 = 6.96958 loss)
I0329 22:49:04.014327 16096 solver.cpp:219] Iteration 1500 (0.548366 iter/s, 36.472s/20 iters), loss = 6.84897
I0329 22:49:04.014416 16096 solver.cpp:238]     Train net output #0: loss = 6.84897 (* 1 = 6.84897 loss)
I0329 22:49:04.014431 16096 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I0329 22:49:23.670150 16096 solver.cpp:219] Iteration 1520 (1.01753 iter/s, 19.6554s/20 iters), loss = 6.86246
I0329 22:49:23.693765 16096 solver.cpp:238]     Train net output #0: loss = 6.86246 (* 1 = 6.86246 loss)
I0329 22:49:23.693797 16096 sgd_solver.cpp:105] Iteration 1520, lr = 0.1
I0329 22:49:43.366391 16096 solver.cpp:219] Iteration 1540 (1.01666 iter/s, 19.6723s/20 iters), loss = 6.83934
I0329 22:49:43.390012 16096 solver.cpp:238]     Train net output #0: loss = 6.83934 (* 1 = 6.83934 loss)
I0329 22:49:43.390041 16096 sgd_solver.cpp:105] Iteration 1540, lr = 0.1
I0329 22:50:03.032323 16096 solver.cpp:219] Iteration 1560 (1.01823 iter/s, 19.642s/20 iters), loss = 6.85796
I0329 22:50:03.055943 16096 solver.cpp:238]     Train net output #0: loss = 6.85796 (* 1 = 6.85796 loss)
I0329 22:50:03.055976 16096 sgd_solver.cpp:105] Iteration 1560, lr = 0.1
I0329 22:50:22.730902 16096 solver.cpp:219] Iteration 1580 (1.01654 iter/s, 19.6747s/20 iters), loss = 6.85001
I0329 22:50:22.754521 16096 solver.cpp:238]     Train net output #0: loss = 6.85001 (* 1 = 6.85001 loss)
I0329 22:50:22.754550 16096 sgd_solver.cpp:105] Iteration 1580, lr = 0.1
I0329 22:50:40.765854 16096 solver.cpp:331] Iteration 1600, Testing net (#0)
I0329 22:50:56.416889 16096 solver.cpp:398]     Test net output #0: accuracy = 0.001
I0329 22:50:56.417114 16096 solver.cpp:398]     Test net output #1: loss = 6.96454 (* 1 = 6.96454 loss)
I0329 22:50:57.377900 16096 solver.cpp:219] Iteration 1600 (0.577653 iter/s, 34.6228s/20 iters), loss = 6.84692
I0329 22:50:57.382607 16096 solver.cpp:238]     Train net output #0: loss = 6.84692 (* 1 = 6.84692 loss)
I0329 22:50:57.382643 16096 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I0329 22:51:17.374323 16096 solver.cpp:219] Iteration 1620 (1.00043 iter/s, 19.9914s/20 iters), loss = 6.84987
I0329 22:51:17.397946 16096 solver.cpp:238]     Train net output #0: loss = 6.84987 (* 1 = 6.84987 loss)
I0329 22:51:17.397977 16096 sgd_solver.cpp:105] Iteration 1620, lr = 0.1
I0329 22:51:37.127918 16096 solver.cpp:219] Iteration 1640 (1.0137 iter/s, 19.7297s/20 iters), loss = 6.84981
I0329 22:51:37.151530 16096 solver.cpp:238]     Train net output #0: loss = 6.84981 (* 1 = 6.84981 loss)
I0329 22:51:37.151559 16096 sgd_solver.cpp:105] Iteration 1640, lr = 0.1
I0329 22:51:56.836319 16096 solver.cpp:219] Iteration 1660 (1.01603 iter/s, 19.6845s/20 iters), loss = 6.82667
I0329 22:51:56.859937 16096 solver.cpp:238]     Train net output #0: loss = 6.82667 (* 1 = 6.82667 loss)
I0329 22:51:56.859971 16096 sgd_solver.cpp:105] Iteration 1660, lr = 0.1
I0329 22:52:16.534199 16096 solver.cpp:219] Iteration 1680 (1.01657 iter/s, 19.6739s/20 iters), loss = 6.84102
I0329 22:52:16.557821 16096 solver.cpp:238]     Train net output #0: loss = 6.84102 (* 1 = 6.84102 loss)
I0329 22:52:16.557874 16096 sgd_solver.cpp:105] Iteration 1680, lr = 0.1
I0329 22:52:34.585863 16096 solver.cpp:331] Iteration 1700, Testing net (#0)
I0329 22:52:50.327817 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0006
I0329 22:52:50.327965 16096 solver.cpp:398]     Test net output #1: loss = 6.9662 (* 1 = 6.9662 loss)
I0329 22:52:51.293004 16096 solver.cpp:219] Iteration 1700 (0.575794 iter/s, 34.7346s/20 iters), loss = 6.83441
I0329 22:52:51.293088 16096 solver.cpp:238]     Train net output #0: loss = 6.83441 (* 1 = 6.83441 loss)
I0329 22:52:51.293102 16096 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I0329 22:53:10.987041 16096 solver.cpp:219] Iteration 1720 (1.01556 iter/s, 19.6936s/20 iters), loss = 6.85932
I0329 22:53:11.010658 16096 solver.cpp:238]     Train net output #0: loss = 6.85932 (* 1 = 6.85932 loss)
I0329 22:53:11.010689 16096 sgd_solver.cpp:105] Iteration 1720, lr = 0.1
I0329 22:53:30.673027 16096 solver.cpp:219] Iteration 1740 (1.01719 iter/s, 19.6621s/20 iters), loss = 6.84982
I0329 22:53:30.696647 16096 solver.cpp:238]     Train net output #0: loss = 6.84982 (* 1 = 6.84982 loss)
I0329 22:53:30.696677 16096 sgd_solver.cpp:105] Iteration 1740, lr = 0.1
I0329 22:53:50.369719 16096 solver.cpp:219] Iteration 1760 (1.01663 iter/s, 19.6728s/20 iters), loss = 6.84594
I0329 22:53:50.393342 16096 solver.cpp:238]     Train net output #0: loss = 6.84594 (* 1 = 6.84594 loss)
I0329 22:53:50.393373 16096 sgd_solver.cpp:105] Iteration 1760, lr = 0.1
I0329 22:54:10.052062 16096 solver.cpp:219] Iteration 1780 (1.01738 iter/s, 19.6584s/20 iters), loss = 6.83713
I0329 22:54:10.075676 16096 solver.cpp:238]     Train net output #0: loss = 6.83713 (* 1 = 6.83713 loss)
I0329 22:54:10.075706 16096 sgd_solver.cpp:105] Iteration 1780, lr = 0.1
I0329 22:54:28.058522 16096 solver.cpp:331] Iteration 1800, Testing net (#0)
I0329 22:54:30.685086 16096 blocking_queue.cpp:49] Waiting for data
I0329 22:54:45.138288 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 22:54:45.138506 16096 solver.cpp:398]     Test net output #1: loss = 6.97888 (* 1 = 6.97888 loss)
I0329 22:54:46.103526 16096 solver.cpp:219] Iteration 1800 (0.555135 iter/s, 36.0273s/20 iters), loss = 6.84251
I0329 22:54:46.103615 16096 solver.cpp:238]     Train net output #0: loss = 6.84251 (* 1 = 6.84251 loss)
I0329 22:54:46.103627 16096 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I0329 22:55:06.132437 16096 solver.cpp:219] Iteration 1820 (0.998577 iter/s, 20.0285s/20 iters), loss = 6.84173
I0329 22:55:06.132527 16096 solver.cpp:238]     Train net output #0: loss = 6.84173 (* 1 = 6.84173 loss)
I0329 22:55:06.132540 16096 sgd_solver.cpp:105] Iteration 1820, lr = 0.1
I0329 22:55:26.050631 16096 solver.cpp:219] Iteration 1840 (1.00413 iter/s, 19.9178s/20 iters), loss = 6.82893
I0329 22:55:26.074275 16096 solver.cpp:238]     Train net output #0: loss = 6.82893 (* 1 = 6.82893 loss)
I0329 22:55:26.074306 16096 sgd_solver.cpp:105] Iteration 1840, lr = 0.1
I0329 22:55:45.723022 16096 solver.cpp:219] Iteration 1860 (1.01789 iter/s, 19.6484s/20 iters), loss = 6.82662
I0329 22:55:45.746634 16096 solver.cpp:238]     Train net output #0: loss = 6.82662 (* 1 = 6.82662 loss)
I0329 22:55:45.746668 16096 sgd_solver.cpp:105] Iteration 1860, lr = 0.1
I0329 22:56:05.393406 16096 solver.cpp:219] Iteration 1880 (1.01799 iter/s, 19.6465s/20 iters), loss = 6.82266
I0329 22:56:05.417021 16096 solver.cpp:238]     Train net output #0: loss = 6.82266 (* 1 = 6.82266 loss)
I0329 22:56:05.417052 16096 sgd_solver.cpp:105] Iteration 1880, lr = 0.1
I0329 22:56:23.426875 16096 solver.cpp:331] Iteration 1900, Testing net (#0)
I0329 22:56:43.027271 16109 data_layer.cpp:73] Restarting data prefetching from start.
I0329 22:56:43.108760 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0006
I0329 22:56:43.108824 16096 solver.cpp:398]     Test net output #1: loss = 6.99301 (* 1 = 6.99301 loss)
I0329 22:56:44.073855 16096 solver.cpp:219] Iteration 1900 (0.517381 iter/s, 38.6562s/20 iters), loss = 6.84395
I0329 22:56:44.073937 16096 solver.cpp:238]     Train net output #0: loss = 6.84395 (* 1 = 6.84395 loss)
I0329 22:56:44.073966 16096 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I0329 22:57:04.096751 16096 solver.cpp:219] Iteration 1920 (0.998876 iter/s, 20.0225s/20 iters), loss = 6.82102
I0329 22:57:04.096832 16096 solver.cpp:238]     Train net output #0: loss = 6.82102 (* 1 = 6.82102 loss)
I0329 22:57:04.096844 16096 sgd_solver.cpp:105] Iteration 1920, lr = 0.1
I0329 22:57:24.060514 16096 solver.cpp:219] Iteration 1940 (1.00184 iter/s, 19.9633s/20 iters), loss = 6.81889
I0329 22:57:24.084179 16096 solver.cpp:238]     Train net output #0: loss = 6.81889 (* 1 = 6.81889 loss)
I0329 22:57:24.084254 16096 sgd_solver.cpp:105] Iteration 1940, lr = 0.1
I0329 22:57:43.797821 16096 solver.cpp:219] Iteration 1960 (1.01454 iter/s, 19.7134s/20 iters), loss = 6.83259
I0329 22:57:43.821439 16096 solver.cpp:238]     Train net output #0: loss = 6.83259 (* 1 = 6.83259 loss)
I0329 22:57:43.821467 16096 sgd_solver.cpp:105] Iteration 1960, lr = 0.1
I0329 22:58:03.484503 16096 solver.cpp:219] Iteration 1980 (1.01715 iter/s, 19.6628s/20 iters), loss = 6.8465
I0329 22:58:03.508124 16096 solver.cpp:238]     Train net output #0: loss = 6.8465 (* 1 = 6.8465 loss)
I0329 22:58:03.508154 16096 sgd_solver.cpp:105] Iteration 1980, lr = 0.1
I0329 22:58:21.519510 16096 solver.cpp:331] Iteration 2000, Testing net (#0)
I0329 22:58:29.182431 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 22:58:29.182507 16096 solver.cpp:398]     Test net output #1: loss = 6.97771 (* 1 = 6.97771 loss)
I0329 22:58:30.150894 16096 solver.cpp:219] Iteration 2000 (0.750684 iter/s, 26.6424s/20 iters), loss = 6.83765
I0329 22:58:30.150972 16096 solver.cpp:238]     Train net output #0: loss = 6.83765 (* 1 = 6.83765 loss)
I0329 22:58:30.150985 16096 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I0329 22:58:50.147388 16096 solver.cpp:219] Iteration 2020 (1.0002 iter/s, 19.9961s/20 iters), loss = 6.84431
I0329 22:58:50.171008 16096 solver.cpp:238]     Train net output #0: loss = 6.84431 (* 1 = 6.84431 loss)
I0329 22:58:50.171036 16096 sgd_solver.cpp:105] Iteration 2020, lr = 0.1
I0329 22:59:09.923385 16096 solver.cpp:219] Iteration 2040 (1.01255 iter/s, 19.7521s/20 iters), loss = 6.84019
I0329 22:59:09.946995 16096 solver.cpp:238]     Train net output #0: loss = 6.84019 (* 1 = 6.84019 loss)
I0329 22:59:09.947026 16096 sgd_solver.cpp:105] Iteration 2040, lr = 0.1
I0329 22:59:29.611856 16096 solver.cpp:219] Iteration 2060 (1.01706 iter/s, 19.6645s/20 iters), loss = 6.85668
I0329 22:59:29.635479 16096 solver.cpp:238]     Train net output #0: loss = 6.85668 (* 1 = 6.85668 loss)
I0329 22:59:29.635509 16096 sgd_solver.cpp:105] Iteration 2060, lr = 0.1
I0329 22:59:49.310053 16096 solver.cpp:219] Iteration 2080 (1.01656 iter/s, 19.6743s/20 iters), loss = 6.82018
I0329 22:59:49.333667 16096 solver.cpp:238]     Train net output #0: loss = 6.82018 (* 1 = 6.82018 loss)
I0329 22:59:49.333699 16096 sgd_solver.cpp:105] Iteration 2080, lr = 0.1
I0329 23:00:07.363149 16096 solver.cpp:331] Iteration 2100, Testing net (#0)
I0329 23:00:30.558156 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0012
I0329 23:00:30.558223 16096 solver.cpp:398]     Test net output #1: loss = 6.98812 (* 1 = 6.98812 loss)
I0329 23:00:31.524497 16096 solver.cpp:219] Iteration 2100 (0.474044 iter/s, 42.1902s/20 iters), loss = 6.83282
I0329 23:00:31.524575 16096 solver.cpp:238]     Train net output #0: loss = 6.83282 (* 1 = 6.83282 loss)
I0329 23:00:31.524588 16096 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I0329 23:00:51.895838 16096 solver.cpp:219] Iteration 2120 (0.981791 iter/s, 20.3709s/20 iters), loss = 6.84314
I0329 23:00:51.919447 16096 solver.cpp:238]     Train net output #0: loss = 6.84314 (* 1 = 6.84314 loss)
I0329 23:00:51.919477 16096 sgd_solver.cpp:105] Iteration 2120, lr = 0.1
I0329 23:01:11.585247 16096 solver.cpp:219] Iteration 2140 (1.01701 iter/s, 19.6655s/20 iters), loss = 6.85862
I0329 23:01:11.608860 16096 solver.cpp:238]     Train net output #0: loss = 6.85862 (* 1 = 6.85862 loss)
I0329 23:01:11.608906 16096 sgd_solver.cpp:105] Iteration 2140, lr = 0.1
I0329 23:01:31.266108 16096 solver.cpp:219] Iteration 2160 (1.01745 iter/s, 19.6569s/20 iters), loss = 6.8435
I0329 23:01:31.289722 16096 solver.cpp:238]     Train net output #0: loss = 6.8435 (* 1 = 6.8435 loss)
I0329 23:01:31.289750 16096 sgd_solver.cpp:105] Iteration 2160, lr = 0.1
I0329 23:01:50.938277 16096 solver.cpp:219] Iteration 2180 (1.0179 iter/s, 19.6483s/20 iters), loss = 6.84672
I0329 23:01:50.961891 16096 solver.cpp:238]     Train net output #0: loss = 6.84672 (* 1 = 6.84672 loss)
I0329 23:01:50.961922 16096 sgd_solver.cpp:105] Iteration 2180, lr = 0.1
I0329 23:02:08.977435 16096 solver.cpp:331] Iteration 2200, Testing net (#0)
I0329 23:02:24.104661 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0018
I0329 23:02:24.104732 16096 solver.cpp:398]     Test net output #1: loss = 6.99191 (* 1 = 6.99191 loss)
I0329 23:02:25.072409 16096 solver.cpp:219] Iteration 2200 (0.586338 iter/s, 34.11s/20 iters), loss = 6.8433
I0329 23:02:25.072494 16096 solver.cpp:238]     Train net output #0: loss = 6.8433 (* 1 = 6.8433 loss)
I0329 23:02:25.072506 16096 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I0329 23:02:44.732910 16096 solver.cpp:219] Iteration 2220 (1.01729 iter/s, 19.6601s/20 iters), loss = 6.84638
I0329 23:02:44.756521 16096 solver.cpp:238]     Train net output #0: loss = 6.84638 (* 1 = 6.84638 loss)
I0329 23:02:44.756556 16096 sgd_solver.cpp:105] Iteration 2220, lr = 0.1
I0329 23:03:04.418660 16096 solver.cpp:219] Iteration 2240 (1.0172 iter/s, 19.6618s/20 iters), loss = 6.82937
I0329 23:03:04.442279 16096 solver.cpp:238]     Train net output #0: loss = 6.82937 (* 1 = 6.82937 loss)
I0329 23:03:04.442311 16096 sgd_solver.cpp:105] Iteration 2240, lr = 0.1
I0329 23:03:24.114063 16096 solver.cpp:219] Iteration 2260 (1.0167 iter/s, 19.6715s/20 iters), loss = 6.84133
I0329 23:03:24.137678 16096 solver.cpp:238]     Train net output #0: loss = 6.84133 (* 1 = 6.84133 loss)
I0329 23:03:24.137709 16096 sgd_solver.cpp:105] Iteration 2260, lr = 0.1
I0329 23:03:43.816102 16096 solver.cpp:219] Iteration 2280 (1.01636 iter/s, 19.6781s/20 iters), loss = 6.82269
I0329 23:03:43.839711 16096 solver.cpp:238]     Train net output #0: loss = 6.82269 (* 1 = 6.82269 loss)
I0329 23:03:43.839741 16096 sgd_solver.cpp:105] Iteration 2280, lr = 0.1
I0329 23:04:01.846784 16096 solver.cpp:331] Iteration 2300, Testing net (#0)
I0329 23:04:22.320186 16096 solver.cpp:398]     Test net output #0: accuracy = 0.001
I0329 23:04:22.320277 16096 solver.cpp:398]     Test net output #1: loss = 6.99547 (* 1 = 6.99547 loss)
I0329 23:04:23.286842 16096 solver.cpp:219] Iteration 2300 (0.507016 iter/s, 39.4465s/20 iters), loss = 6.82981
I0329 23:04:23.286931 16096 solver.cpp:238]     Train net output #0: loss = 6.82981 (* 1 = 6.82981 loss)
I0329 23:04:23.286943 16096 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I0329 23:04:43.124183 16096 solver.cpp:219] Iteration 2320 (1.00822 iter/s, 19.8369s/20 iters), loss = 6.83043
I0329 23:04:43.147792 16096 solver.cpp:238]     Train net output #0: loss = 6.83043 (* 1 = 6.83043 loss)
I0329 23:04:43.147824 16096 sgd_solver.cpp:105] Iteration 2320, lr = 0.1
I0329 23:05:02.818213 16096 solver.cpp:219] Iteration 2340 (1.01677 iter/s, 19.6701s/20 iters), loss = 6.8309
I0329 23:05:02.841826 16096 solver.cpp:238]     Train net output #0: loss = 6.8309 (* 1 = 6.8309 loss)
I0329 23:05:02.841857 16096 sgd_solver.cpp:105] Iteration 2340, lr = 0.1
I0329 23:05:22.507325 16096 solver.cpp:219] Iteration 2360 (1.01703 iter/s, 19.6652s/20 iters), loss = 6.8502
I0329 23:05:22.530946 16096 solver.cpp:238]     Train net output #0: loss = 6.8502 (* 1 = 6.8502 loss)
I0329 23:05:22.530975 16096 sgd_solver.cpp:105] Iteration 2360, lr = 0.1
I0329 23:05:42.187530 16096 solver.cpp:219] Iteration 2380 (1.01749 iter/s, 19.6563s/20 iters), loss = 6.85515
I0329 23:05:42.211154 16096 solver.cpp:238]     Train net output #0: loss = 6.85515 (* 1 = 6.85515 loss)
I0329 23:05:42.211185 16096 sgd_solver.cpp:105] Iteration 2380, lr = 0.1
I0329 23:06:00.227658 16096 solver.cpp:331] Iteration 2400, Testing net (#0)
I0329 23:06:20.600028 16096 blocking_queue.cpp:49] Waiting for data
I0329 23:06:26.208943 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 23:06:26.209018 16096 solver.cpp:398]     Test net output #1: loss = 6.98829 (* 1 = 6.98829 loss)
I0329 23:06:27.174455 16096 solver.cpp:219] Iteration 2400 (0.444814 iter/s, 44.9626s/20 iters), loss = 6.86819
I0329 23:06:27.174542 16096 solver.cpp:238]     Train net output #0: loss = 6.86819 (* 1 = 6.86819 loss)
I0329 23:06:27.174554 16096 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I0329 23:06:46.916072 16096 solver.cpp:219] Iteration 2420 (1.01311 iter/s, 19.7412s/20 iters), loss = 6.83571
I0329 23:06:46.939709 16096 solver.cpp:238]     Train net output #0: loss = 6.83571 (* 1 = 6.83571 loss)
I0329 23:06:46.939739 16096 sgd_solver.cpp:105] Iteration 2420, lr = 0.1
I0329 23:07:06.594699 16096 solver.cpp:219] Iteration 2440 (1.01757 iter/s, 19.6547s/20 iters), loss = 6.84793
I0329 23:07:06.618316 16096 solver.cpp:238]     Train net output #0: loss = 6.84793 (* 1 = 6.84793 loss)
I0329 23:07:06.618346 16096 sgd_solver.cpp:105] Iteration 2440, lr = 0.1
I0329 23:07:26.270934 16096 solver.cpp:219] Iteration 2460 (1.01769 iter/s, 19.6523s/20 iters), loss = 6.8277
I0329 23:07:26.294551 16096 solver.cpp:238]     Train net output #0: loss = 6.8277 (* 1 = 6.8277 loss)
I0329 23:07:26.294579 16096 sgd_solver.cpp:105] Iteration 2460, lr = 0.1
I0329 23:07:45.948047 16096 solver.cpp:219] Iteration 2480 (1.01765 iter/s, 19.6532s/20 iters), loss = 6.84163
I0329 23:07:45.971668 16096 solver.cpp:238]     Train net output #0: loss = 6.84163 (* 1 = 6.84163 loss)
I0329 23:07:45.971699 16096 sgd_solver.cpp:105] Iteration 2480, lr = 0.1
I0329 23:08:03.983664 16096 solver.cpp:331] Iteration 2500, Testing net (#0)
I0329 23:08:21.473820 16096 solver.cpp:398]     Test net output #0: accuracy = 0.001
I0329 23:08:21.473906 16096 solver.cpp:398]     Test net output #1: loss = 6.99189 (* 1 = 6.99189 loss)
I0329 23:08:22.439190 16096 solver.cpp:219] Iteration 2500 (0.548442 iter/s, 36.467s/20 iters), loss = 6.83691
I0329 23:08:22.443850 16096 solver.cpp:238]     Train net output #0: loss = 6.83691 (* 1 = 6.83691 loss)
I0329 23:08:22.443886 16096 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I0329 23:08:42.118480 16096 solver.cpp:219] Iteration 2520 (1.01655 iter/s, 19.6743s/20 iters), loss = 6.81406
I0329 23:08:42.142102 16096 solver.cpp:238]     Train net output #0: loss = 6.81406 (* 1 = 6.81406 loss)
I0329 23:08:42.142133 16096 sgd_solver.cpp:105] Iteration 2520, lr = 0.1
I0329 23:09:01.798024 16096 solver.cpp:219] Iteration 2540 (1.01752 iter/s, 19.6556s/20 iters), loss = 6.84034
I0329 23:09:01.821636 16096 solver.cpp:238]     Train net output #0: loss = 6.84034 (* 1 = 6.84034 loss)
I0329 23:09:01.821668 16096 sgd_solver.cpp:105] Iteration 2540, lr = 0.1
I0329 23:09:21.471796 16096 solver.cpp:219] Iteration 2560 (1.01782 iter/s, 19.6499s/20 iters), loss = 6.85228
I0329 23:09:21.495407 16096 solver.cpp:238]     Train net output #0: loss = 6.85228 (* 1 = 6.85228 loss)
I0329 23:09:21.495437 16096 sgd_solver.cpp:105] Iteration 2560, lr = 0.1
I0329 23:09:41.158483 16096 solver.cpp:219] Iteration 2580 (1.01715 iter/s, 19.6628s/20 iters), loss = 6.82835
I0329 23:09:41.182097 16096 solver.cpp:238]     Train net output #0: loss = 6.82835 (* 1 = 6.82835 loss)
I0329 23:09:41.182130 16096 sgd_solver.cpp:105] Iteration 2580, lr = 0.1
I0329 23:09:59.221621 16096 solver.cpp:331] Iteration 2600, Testing net (#0)
I0329 23:10:16.834040 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 23:10:16.834139 16096 solver.cpp:398]     Test net output #1: loss = 6.98812 (* 1 = 6.98812 loss)
I0329 23:10:17.800971 16096 solver.cpp:219] Iteration 2600 (0.546175 iter/s, 36.6183s/20 iters), loss = 6.83875
I0329 23:10:17.801056 16096 solver.cpp:238]     Train net output #0: loss = 6.83875 (* 1 = 6.83875 loss)
I0329 23:10:17.801069 16096 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I0329 23:10:37.519155 16096 solver.cpp:219] Iteration 2620 (1.01431 iter/s, 19.7178s/20 iters), loss = 6.84618
I0329 23:10:37.542773 16096 solver.cpp:238]     Train net output #0: loss = 6.84618 (* 1 = 6.84618 loss)
I0329 23:10:37.542800 16096 sgd_solver.cpp:105] Iteration 2620, lr = 0.1
I0329 23:10:57.215561 16096 solver.cpp:219] Iteration 2640 (1.01665 iter/s, 19.6725s/20 iters), loss = 6.82431
I0329 23:10:57.239173 16096 solver.cpp:238]     Train net output #0: loss = 6.82431 (* 1 = 6.82431 loss)
I0329 23:10:57.239205 16096 sgd_solver.cpp:105] Iteration 2640, lr = 0.1
I0329 23:11:16.910506 16096 solver.cpp:219] Iteration 2660 (1.01672 iter/s, 19.671s/20 iters), loss = 6.83859
I0329 23:11:16.934123 16096 solver.cpp:238]     Train net output #0: loss = 6.83859 (* 1 = 6.83859 loss)
I0329 23:11:16.934154 16096 sgd_solver.cpp:105] Iteration 2660, lr = 0.1
I0329 23:11:36.593813 16096 solver.cpp:219] Iteration 2680 (1.01733 iter/s, 19.6594s/20 iters), loss = 6.84667
I0329 23:11:36.617429 16096 solver.cpp:238]     Train net output #0: loss = 6.84667 (* 1 = 6.84667 loss)
I0329 23:11:36.617458 16096 sgd_solver.cpp:105] Iteration 2680, lr = 0.1
I0329 23:11:54.642037 16096 solver.cpp:331] Iteration 2700, Testing net (#0)
I0329 23:12:10.201254 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0008
I0329 23:12:10.201333 16096 solver.cpp:398]     Test net output #1: loss = 6.98698 (* 1 = 6.98698 loss)
I0329 23:12:11.165446 16096 solver.cpp:219] Iteration 2700 (0.578913 iter/s, 34.5475s/20 iters), loss = 6.82726
I0329 23:12:11.165534 16096 solver.cpp:238]     Train net output #0: loss = 6.82726 (* 1 = 6.82726 loss)
I0329 23:12:11.165546 16096 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I0329 23:12:31.000074 16096 solver.cpp:219] Iteration 2720 (1.00836 iter/s, 19.8342s/20 iters), loss = 6.84063
I0329 23:12:31.023689 16096 solver.cpp:238]     Train net output #0: loss = 6.84063 (* 1 = 6.84063 loss)
I0329 23:12:31.023716 16096 sgd_solver.cpp:105] Iteration 2720, lr = 0.1
I0329 23:12:50.708271 16096 solver.cpp:219] Iteration 2740 (1.01604 iter/s, 19.6843s/20 iters), loss = 6.82843
I0329 23:12:50.731881 16096 solver.cpp:238]     Train net output #0: loss = 6.82843 (* 1 = 6.82843 loss)
I0329 23:12:50.731914 16096 sgd_solver.cpp:105] Iteration 2740, lr = 0.1
I0329 23:13:10.425822 16096 solver.cpp:219] Iteration 2760 (1.01556 iter/s, 19.6936s/20 iters), loss = 6.82807
I0329 23:13:10.449431 16096 solver.cpp:238]     Train net output #0: loss = 6.82807 (* 1 = 6.82807 loss)
I0329 23:13:10.449460 16096 sgd_solver.cpp:105] Iteration 2760, lr = 0.1
I0329 23:13:30.124497 16096 solver.cpp:219] Iteration 2780 (1.01653 iter/s, 19.6748s/20 iters), loss = 6.83282
I0329 23:13:30.148118 16096 solver.cpp:238]     Train net output #0: loss = 6.83282 (* 1 = 6.83282 loss)
I0329 23:13:30.148149 16096 sgd_solver.cpp:105] Iteration 2780, lr = 0.1
I0329 23:13:48.180145 16096 solver.cpp:331] Iteration 2800, Testing net (#0)
I0329 23:14:06.645251 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0006
I0329 23:14:06.645334 16096 solver.cpp:398]     Test net output #1: loss = 7.00272 (* 1 = 7.00272 loss)
I0329 23:14:07.613291 16096 solver.cpp:219] Iteration 2800 (0.533837 iter/s, 37.4646s/20 iters), loss = 6.83865
I0329 23:14:07.613378 16096 solver.cpp:238]     Train net output #0: loss = 6.83865 (* 1 = 6.83865 loss)
I0329 23:14:07.613390 16096 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I0329 23:14:27.302273 16096 solver.cpp:219] Iteration 2820 (1.01582 iter/s, 19.6886s/20 iters), loss = 6.81711
I0329 23:14:27.325899 16096 solver.cpp:238]     Train net output #0: loss = 6.81711 (* 1 = 6.81711 loss)
I0329 23:14:27.325929 16096 sgd_solver.cpp:105] Iteration 2820, lr = 0.1
I0329 23:14:46.983244 16096 solver.cpp:219] Iteration 2840 (1.01745 iter/s, 19.657s/20 iters), loss = 6.81717
I0329 23:14:47.006865 16096 solver.cpp:238]     Train net output #0: loss = 6.81717 (* 1 = 6.81717 loss)
I0329 23:14:47.006897 16096 sgd_solver.cpp:105] Iteration 2840, lr = 0.1
I0329 23:15:06.702769 16096 solver.cpp:219] Iteration 2860 (1.01546 iter/s, 19.6956s/20 iters), loss = 6.81111
I0329 23:15:06.726384 16096 solver.cpp:238]     Train net output #0: loss = 6.81111 (* 1 = 6.81111 loss)
I0329 23:15:06.726439 16096 sgd_solver.cpp:105] Iteration 2860, lr = 0.1
I0329 23:15:26.409953 16096 solver.cpp:219] Iteration 2880 (1.01609 iter/s, 19.6833s/20 iters), loss = 6.83141
I0329 23:15:26.433568 16096 solver.cpp:238]     Train net output #0: loss = 6.83141 (* 1 = 6.83141 loss)
I0329 23:15:26.433601 16096 sgd_solver.cpp:105] Iteration 2880, lr = 0.1
I0329 23:15:44.481922 16096 solver.cpp:331] Iteration 2900, Testing net (#0)
I0329 23:16:04.183979 16109 data_layer.cpp:73] Restarting data prefetching from start.
I0329 23:16:04.265290 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0016
I0329 23:16:04.265347 16096 solver.cpp:398]     Test net output #1: loss = 7.01466 (* 1 = 7.01466 loss)
I0329 23:16:05.231204 16096 solver.cpp:219] Iteration 2900 (0.515503 iter/s, 38.797s/20 iters), loss = 6.83679
I0329 23:16:05.231287 16096 solver.cpp:238]     Train net output #0: loss = 6.83679 (* 1 = 6.83679 loss)
I0329 23:16:05.231299 16096 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I0329 23:16:25.056499 16096 solver.cpp:219] Iteration 2920 (1.00883 iter/s, 19.8249s/20 iters), loss = 6.83861
I0329 23:16:25.080126 16096 solver.cpp:238]     Train net output #0: loss = 6.83861 (* 1 = 6.83861 loss)
I0329 23:16:25.080157 16096 sgd_solver.cpp:105] Iteration 2920, lr = 0.1
I0329 23:16:44.765861 16096 solver.cpp:219] Iteration 2940 (1.01598 iter/s, 19.6854s/20 iters), loss = 6.82439
I0329 23:16:44.789485 16096 solver.cpp:238]     Train net output #0: loss = 6.82439 (* 1 = 6.82439 loss)
I0329 23:16:44.789515 16096 sgd_solver.cpp:105] Iteration 2940, lr = 0.1
I0329 23:17:04.484385 16096 solver.cpp:219] Iteration 2960 (1.01551 iter/s, 19.6946s/20 iters), loss = 6.83155
I0329 23:17:04.508003 16096 solver.cpp:238]     Train net output #0: loss = 6.83155 (* 1 = 6.83155 loss)
I0329 23:17:04.508033 16096 sgd_solver.cpp:105] Iteration 2960, lr = 0.1
I0329 23:17:24.177038 16096 solver.cpp:219] Iteration 2980 (1.01684 iter/s, 19.6687s/20 iters), loss = 6.81078
I0329 23:17:24.200649 16096 solver.cpp:238]     Train net output #0: loss = 6.81078 (* 1 = 6.81078 loss)
I0329 23:17:24.200680 16096 sgd_solver.cpp:105] Iteration 2980, lr = 0.1
I0329 23:17:29.430773 16096 blocking_queue.cpp:49] Waiting for data
I0329 23:17:42.219321 16096 solver.cpp:448] Snapshotting to binary proto file models/caffenet_proj/caffenet_train_iter_3000.caffemodel
I0329 23:17:50.160471 16096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/caffenet_proj/caffenet_train_iter_3000.solverstate
I0329 23:17:52.688129 16096 solver.cpp:311] Iteration 3000, loss = 6.82319
I0329 23:17:53.904342 16096 solver.cpp:331] Iteration 3000, Testing net (#0)
I0329 23:18:00.800616 16096 solver.cpp:398]     Test net output #0: accuracy = 0.0006
I0329 23:18:00.800691 16096 solver.cpp:398]     Test net output #1: loss = 6.99709 (* 1 = 6.99709 loss)
I0329 23:18:00.800701 16096 solver.cpp:316] Optimization Done.
I0329 23:18:00.800707 16096 caffe.cpp:259] Optimization Done.
